[2024-05-08 16:03:40,740] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-08 16:03:44,970] [INFO] [runner.py:463:main] Using IP address of 192.168.1.17 for node h800-l05
[2024-05-08 16:03:44,971] [INFO] [multinode_runner.py:72:get_cmd] Running on the following workers: h800-l05,h800-l07,h800-l09,h800-l03
[2024-05-08 16:03:44,971] [INFO] [runner.py:570:main] cmd = pdsh -S -f 1024 -w h800-l05,h800-l07,h800-l09,h800-l03 export PYTHONPATH=/data/nvme3/Megatron-DeepSpeed-patch;  cd /data/nvme3/Megatron-DeepSpeed-patch; /home/cestc/miniconda3/envs/ds/bin/python -u -m deepspeed.launcher.launch --world_info=eyJoODAwLWwwNSI6IFswLCAxLCAyLCAzLCA0LCA1LCA2LCA3XSwgImg4MDAtbDA3IjogWzAsIDEsIDIsIDMsIDQsIDUsIDYsIDddLCAiaDgwMC1sMDkiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN10sICJoODAwLWwwMyI6IFswLCAxLCAyLCAzLCA0LCA1LCA2LCA3XX0= --node_rank=%n --master_addr=192.168.1.17 --master_port=29500 /data/nvme3/Megatron-DeepSpeed-patch/pretrain_gpt.py --tensor-model-parallel-size '1' --pipeline-model-parallel-size '8' --num-layers '80' --hidden-size '8192' --num-attention-heads '64' --seq-length '8192' --loss-scale '12' --max-position-embeddings '8192' --micro-batch-size '1' --global-batch-size '128' --train-iters '200000' --lr '2e-4' --min-lr '2.0e-5' --lr-warmup-iters '1000' --lr-decay-style 'cosine' --lr-decay-iters '10000' --log-interval '1' --eval-iters '40' --eval-interval '1000000' --data-path '/share/train_data' --vocab-file './ch_tokenizer_data/vocab.txt' --save './configs' --load './configs' --save-interval '100' --split '800,100,100' --clip-grad '1.0' --optimizer 'adam' --weight-decay '0.1' --adam-beta1 '0.9' --adam-beta2 '0.95' --init-method-std '0.006' --tokenizer-type 'QwenTokenizer' --cpu-optimizer --fp16 --use-flash-attn-v2 --checkpoint-activations --deepspeed --deepspeed_config=/data/nvme3/Megatron-DeepSpeed-patch/ds_config.json --zero-stage=0 --deepspeed-activation-checkpointing
h800-l05: [2024-05-08 16:03:46,516] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
h800-l07: [2024-05-08 16:03:46,710] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
h800-l09: [2024-05-08 16:03:46,688] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
h800-l03: [2024-05-08 16:03:47,223] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
h800-l05: [2024-05-08 16:03:47,475] [INFO] [launch.py:145:main] WORLD INFO DICT: {'h800-l05': [0, 1, 2, 3, 4, 5, 6, 7], 'h800-l07': [0, 1, 2, 3, 4, 5, 6, 7], 'h800-l09': [0, 1, 2, 3, 4, 5, 6, 7], 'h800-l03': [0, 1, 2, 3, 4, 5, 6, 7]}
h800-l05: [2024-05-08 16:03:47,475] [INFO] [launch.py:151:main] nnodes=4, num_local_procs=8, node_rank=0
h800-l05: [2024-05-08 16:03:47,475] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'h800-l05': [0, 1, 2, 3, 4, 5, 6, 7], 'h800-l07': [8, 9, 10, 11, 12, 13, 14, 15], 'h800-l09': [16, 17, 18, 19, 20, 21, 22, 23], 'h800-l03': [24, 25, 26, 27, 28, 29, 30, 31]})
h800-l05: [2024-05-08 16:03:47,475] [INFO] [launch.py:163:main] dist_world_size=32
h800-l05: [2024-05-08 16:03:47,475] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
h800-l05: [2024-05-08 16:03:48,599] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
h800-l05: [2024-05-08 16:03:48,611] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
h800-l05: [2024-05-08 16:03:48,621] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
h800-l05: [2024-05-08 16:03:48,632] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
h800-l05: [2024-05-08 16:03:48,642] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
h800-l05: [2024-05-08 16:03:48,648] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
h800-l05: [2024-05-08 16:03:48,666] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
h800-l05: [2024-05-08 16:03:48,677] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
h800-l07: [2024-05-08 16:03:49,820] [INFO] [launch.py:145:main] WORLD INFO DICT: {'h800-l05': [0, 1, 2, 3, 4, 5, 6, 7], 'h800-l07': [0, 1, 2, 3, 4, 5, 6, 7], 'h800-l09': [0, 1, 2, 3, 4, 5, 6, 7], 'h800-l03': [0, 1, 2, 3, 4, 5, 6, 7]}
h800-l07: [2024-05-08 16:03:49,820] [INFO] [launch.py:151:main] nnodes=4, num_local_procs=8, node_rank=1
h800-l07: [2024-05-08 16:03:49,820] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'h800-l05': [0, 1, 2, 3, 4, 5, 6, 7], 'h800-l07': [8, 9, 10, 11, 12, 13, 14, 15], 'h800-l09': [16, 17, 18, 19, 20, 21, 22, 23], 'h800-l03': [24, 25, 26, 27, 28, 29, 30, 31]})
h800-l07: [2024-05-08 16:03:49,820] [INFO] [launch.py:163:main] dist_world_size=32
h800-l07: [2024-05-08 16:03:49,820] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
h800-l09: [2024-05-08 16:03:49,794] [INFO] [launch.py:145:main] WORLD INFO DICT: {'h800-l05': [0, 1, 2, 3, 4, 5, 6, 7], 'h800-l07': [0, 1, 2, 3, 4, 5, 6, 7], 'h800-l09': [0, 1, 2, 3, 4, 5, 6, 7], 'h800-l03': [0, 1, 2, 3, 4, 5, 6, 7]}
h800-l09: [2024-05-08 16:03:49,794] [INFO] [launch.py:151:main] nnodes=4, num_local_procs=8, node_rank=2
h800-l09: [2024-05-08 16:03:49,794] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'h800-l05': [0, 1, 2, 3, 4, 5, 6, 7], 'h800-l07': [8, 9, 10, 11, 12, 13, 14, 15], 'h800-l09': [16, 17, 18, 19, 20, 21, 22, 23], 'h800-l03': [24, 25, 26, 27, 28, 29, 30, 31]})
h800-l09: [2024-05-08 16:03:49,794] [INFO] [launch.py:163:main] dist_world_size=32
h800-l09: [2024-05-08 16:03:49,794] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
h800-l03: [2024-05-08 16:03:50,933] [INFO] [launch.py:145:main] WORLD INFO DICT: {'h800-l05': [0, 1, 2, 3, 4, 5, 6, 7], 'h800-l07': [0, 1, 2, 3, 4, 5, 6, 7], 'h800-l09': [0, 1, 2, 3, 4, 5, 6, 7], 'h800-l03': [0, 1, 2, 3, 4, 5, 6, 7]}
h800-l03: [2024-05-08 16:03:50,933] [INFO] [launch.py:151:main] nnodes=4, num_local_procs=8, node_rank=3
h800-l03: [2024-05-08 16:03:50,933] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'h800-l05': [0, 1, 2, 3, 4, 5, 6, 7], 'h800-l07': [8, 9, 10, 11, 12, 13, 14, 15], 'h800-l09': [16, 17, 18, 19, 20, 21, 22, 23], 'h800-l03': [24, 25, 26, 27, 28, 29, 30, 31]})
h800-l03: [2024-05-08 16:03:50,934] [INFO] [launch.py:163:main] dist_world_size=32
h800-l03: [2024-05-08 16:03:50,934] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
h800-l07: [2024-05-08 16:03:51,068] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
h800-l07: [2024-05-08 16:03:51,077] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
h800-l07: [2024-05-08 16:03:51,093] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
h800-l09: [2024-05-08 16:03:51,076] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
h800-l09: [2024-05-08 16:03:51,083] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
h800-l07: [2024-05-08 16:03:51,113] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
h800-l09: [2024-05-08 16:03:51,093] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
h800-l09: [2024-05-08 16:03:51,099] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
h800-l07: [2024-05-08 16:03:51,129] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
h800-l09: [2024-05-08 16:03:51,105] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
h800-l07: [2024-05-08 16:03:51,179] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
h800-l09: [2024-05-08 16:03:51,153] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
h800-l09: [2024-05-08 16:03:51,179] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
h800-l09: [2024-05-08 16:03:51,196] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
h800-l07: [2024-05-08 16:03:51,236] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
h800-l07: [2024-05-08 16:03:51,241] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
h800-l05: /data/nvme3/Megatron-DeepSpeed/megatron/model/module.py:13: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /data/nvme5/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
h800-l05:   _FLOAT_TYPES = [get_accelerator().FloatTensor(0).dtype]
h800-l05: --------------------------------------------------
h800-l05: DeepSpeed C++/CUDA extension op report
h800-l05: --------------------------------------------------
h800-l05: NOTE: Ops not installed will be just-in-time (JIT) compiled at
h800-l05:       runtime if needed. Op compatibility means that your system
h800-l05:       meet the required dependencies to JIT install the op.
h800-l05: --------------------------------------------------
h800-l05: JIT compiled ops requires ninja
h800-l05: ninja .................. [92m[OKAY][0m
h800-l05: --------------------------------------------------
h800-l05: op name ................ installed .. compatible
h800-l05: --------------------------------------------------
h800-l05: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
h800-l05: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
h800-l05: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
h800-l05: async_io ............... [93m[NO][0m ....... [93m[NO][0m
h800-l05: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
h800-l05: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
h800-l05: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
h800-l05: [93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
h800-l05: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
h800-l05: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: --------------------------------------------------
h800-l05: DeepSpeed general environment info:
h800-l05: torch install path ............... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/torch']
h800-l05: torch version .................... 2.1.0a0+git7bcf7da
h800-l05: deepspeed install path ........... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/deepspeed']
h800-l05: deepspeed info ................... 0.10.3+04a6fedf, 04a6fedf, HEAD
h800-l05: torch cuda version ............... 12.2
h800-l05: torch hip version ................ None
h800-l05: nvcc version ..................... 12.2
h800-l05: deepspeed wheel compiled w. ...... torch 2.1, cuda 12.2
h800-l05: shared memory (/dev/shm) size .... 755.78 GB
h800-l05: /data/nvme3/Megatron-DeepSpeed/megatron/model/module.py:13: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /data/nvme5/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
h800-l05:   _FLOAT_TYPES = [get_accelerator().FloatTensor(0).dtype]
h800-l05: **** Git info for Megatron: git_hash=a8434f7 git_branch=main ****
h800-l05: --------------------------------------------------
h800-l05: DeepSpeed C++/CUDA extension op report
h800-l05: --------------------------------------------------
h800-l05: NOTE: Ops not installed will be just-in-time (JIT) compiled at
h800-l05:       runtime if needed. Op compatibility means that your system
h800-l05:       meet the required dependencies to JIT install the op.
h800-l05: --------------------------------------------------
h800-l05: JIT compiled ops requires ninja
h800-l05: ninja .................. [92m[OKAY][0m
h800-l05: --------------------------------------------------
h800-l05: op name ................ installed .. compatible
h800-l05: --------------------------------------------------
h800-l05: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
h800-l05: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
h800-l05: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
h800-l05: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
h800-l05: async_io ............... [93m[NO][0m ....... [93m[NO][0m
h800-l05: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
h800-l05: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
h800-l05: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
h800-l05: [93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
h800-l05: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
h800-l05: [2024-05-08 16:03:52,321] [INFO] [comm.py:637:init_distributed] cdb=None
h800-l05: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: --------------------------------------------------
h800-l05: DeepSpeed general environment info:
h800-l05: torch install path ............... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/torch']
h800-l05: torch version .................... 2.1.0a0+git7bcf7da
h800-l05: deepspeed install path ........... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/deepspeed']
h800-l05: deepspeed info ................... 0.10.3+04a6fedf, 04a6fedf, HEAD
h800-l05: torch cuda version ............... 12.2
h800-l05: torch hip version ................ None
h800-l05: nvcc version ..................... 12.2
h800-l05: deepspeed wheel compiled w. ...... torch 2.1, cuda 12.2
h800-l05: shared memory (/dev/shm) size .... 755.78 GB
h800-l05: /data/nvme3/Megatron-DeepSpeed/megatron/model/module.py:13: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /data/nvme5/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
h800-l05:   _FLOAT_TYPES = [get_accelerator().FloatTensor(0).dtype]
h800-l05: /data/nvme3/Megatron-DeepSpeed/megatron/model/module.py:13: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /data/nvme5/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
h800-l05:   _FLOAT_TYPES = [get_accelerator().FloatTensor(0).dtype]
h800-l05: --------------------------------------------------
h800-l05: DeepSpeed C++/CUDA extension op report
h800-l05: --------------------------------------------------
h800-l05: NOTE: Ops not installed will be just-in-time (JIT) compiled at
h800-l05:       runtime if needed. Op compatibility means that your system
h800-l05:       meet the required dependencies to JIT install the op.
h800-l05: --------------------------------------------------
h800-l05: JIT compiled ops requires ninja
h800-l05: ninja .................. [92m[OKAY][0m
h800-l05: --------------------------------------------------
h800-l05: op name ................ installed .. compatible
h800-l05: --------------------------------------------------
h800-l05: /data/nvme3/Megatron-DeepSpeed/megatron/model/module.py:13: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /data/nvme5/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
h800-l05:   _FLOAT_TYPES = [get_accelerator().FloatTensor(0).dtype]
h800-l05: **** Git info for Megatron: git_hash=a8434f7 git_branch=main ****
h800-l05: --------------------------------------------------
h800-l05: DeepSpeed C++/CUDA extension op report
h800-l05: --------------------------------------------------
h800-l05: NOTE: Ops not installed will be just-in-time (JIT) compiled at
h800-l05:       runtime if needed. Op compatibility means that your system
h800-l05:       meet the required dependencies to JIT install the op.
h800-l05: --------------------------------------------------
h800-l05: JIT compiled ops requires ninja
h800-l05: ninja .................. [92m[OKAY][0m
h800-l05: --------------------------------------------------
h800-l05: op name ................ installed .. compatible
h800-l05: --------------------------------------------------
h800-l05: /data/nvme3/Megatron-DeepSpeed/megatron/model/module.py:13: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /data/nvme5/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
h800-l05:   _FLOAT_TYPES = [get_accelerator().FloatTensor(0).dtype]
h800-l05: --------------------------------------------------
h800-l05: DeepSpeed C++/CUDA extension op report
h800-l05: --------------------------------------------------
h800-l05: NOTE: Ops not installed will be just-in-time (JIT) compiled at
h800-l05:       runtime if needed. Op compatibility means that your system
h800-l05:       meet the required dependencies to JIT install the op.
h800-l05: --------------------------------------------------
h800-l05: JIT compiled ops requires ninja
h800-l05: ninja .................. [92m[OKAY][0m
h800-l05: --------------------------------------------------
h800-l05: op name ................ installed .. compatible
h800-l05: --------------------------------------------------
h800-l05: --------------------------------------------------
h800-l05: DeepSpeed C++/CUDA extension op report
h800-l05: --------------------------------------------------
h800-l05: NOTE: Ops not installed will be just-in-time (JIT) compiled at
h800-l05:       runtime if needed. Op compatibility means that your system
h800-l05:       meet the required dependencies to JIT install the op.
h800-l05: --------------------------------------------------
h800-l05: JIT compiled ops requires ninja
h800-l05: ninja .................. [92m[OKAY][0m
h800-l05: --------------------------------------------------
h800-l05: op name ................ installed .. compatible
h800-l05: --------------------------------------------------
h800-l05: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
h800-l05: /data/nvme3/Megatron-DeepSpeed/megatron/model/module.py:13: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /data/nvme5/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
h800-l05:   _FLOAT_TYPES = [get_accelerator().FloatTensor(0).dtype]
h800-l05: --------------------------------------------------
h800-l05: DeepSpeed C++/CUDA extension op report
h800-l05: --------------------------------------------------
h800-l05: NOTE: Ops not installed will be just-in-time (JIT) compiled at
h800-l05:       runtime if needed. Op compatibility means that your system
h800-l05:       meet the required dependencies to JIT install the op.
h800-l05: --------------------------------------------------
h800-l05: JIT compiled ops requires ninja
h800-l05: ninja .................. [92m[OKAY][0m
h800-l05: --------------------------------------------------
h800-l05: op name ................ installed .. compatible
h800-l05: --------------------------------------------------
h800-l05: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
h800-l05: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
h800-l05: /data/nvme3/Megatron-DeepSpeed/megatron/model/module.py:13: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /data/nvme5/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
h800-l05:   _FLOAT_TYPES = [get_accelerator().FloatTensor(0).dtype]
h800-l05: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
h800-l05: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
h800-l05: --------------------------------------------------
h800-l05: DeepSpeed C++/CUDA extension op report
h800-l05: --------------------------------------------------
h800-l05: NOTE: Ops not installed will be just-in-time (JIT) compiled at
h800-l05:       runtime if needed. Op compatibility means that your system
h800-l05:       meet the required dependencies to JIT install the op.
h800-l05: --------------------------------------------------
h800-l05: JIT compiled ops requires ninja
h800-l05: ninja .................. [92m[OKAY][0m
h800-l05: --------------------------------------------------
h800-l05: op name ................ installed .. compatible
h800-l05: --------------------------------------------------
h800-l05: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
h800-l05: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
h800-l05: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
h800-l05: async_io ............... [93m[NO][0m ....... [93m[NO][0m
h800-l05: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
h800-l05: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
h800-l05: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
h800-l05: [93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
h800-l05: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
h800-l05: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
h800-l05: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
h800-l05: async_io ............... [93m[NO][0m ....... [93m[NO][0m
h800-l05: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
h800-l05: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
h800-l05: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
h800-l05: [93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
h800-l05: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
h800-l05: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
h800-l05: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
h800-l05: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
h800-l05: async_io ............... [93m[NO][0m ....... [93m[NO][0m
h800-l05: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
h800-l05: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
h800-l05: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
h800-l05: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
h800-l05: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
h800-l05: async_io ............... [93m[NO][0m ....... [93m[NO][0m
h800-l05: fused_adam ............. [93m[NO][0m .......[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention 
h800-l05: [92m[OKAY][0m
h800-l05: sparse_attn ............cpu_adam  [93m[NO][0m...............  .......[93m[NO][0m  [93m[NO][0m.......
h800-l05:  [92m[OKAY][0m
h800-l05: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
h800-l05: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
h800-l05: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
h800-l05: [93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
h800-l05: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
h800-l05: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: --------------------------------------------------
h800-l05: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: --------------------------------------------------
h800-l05: [2024-05-08 16:03:52,585] [INFO] [comm.py:637:init_distributed] cdb=None
h800-l05: DeepSpeed general environment info:
h800-l05: torch install path ............... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/torch']
h800-l05: torch version .................... 2.1.0a0+git7bcf7da
h800-l05: deepspeed install path ........... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/deepspeed']
h800-l05: deepspeed info ................... 0.10.3+04a6fedf, 04a6fedf, HEAD
h800-l05: torch cuda version ............... 12.2
h800-l05: torch hip version ................ None
h800-l05: nvcc version ..................... 12.2
h800-l05: deepspeed wheel compiled w. ...... torch 2.1, cuda 12.2
h800-l05: shared memory (/dev/shm) size .... 755.78 GB
h800-l05: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: DeepSpeed general environment info:
h800-l05: torch install path ............... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/torch']
h800-l05: torch version .................... 2.1.0a0+git7bcf7da
h800-l05: deepspeed install path ........... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/deepspeed']
h800-l05: deepspeed info ................... 0.10.3+04a6fedf, 04a6fedf, HEAD
h800-l05: torch cuda version ............... 12.2
h800-l05: torch hip version ................ None
h800-l05: nvcc version ..................... 12.2
h800-l05: deepspeed wheel compiled w. ...... torch 2.1, cuda 12.2
h800-l05: shared memory (/dev/shm) size .... 755.78 GB
h800-l05: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
h800-l05: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
h800-l05: async_io ............... [93m[NO][0m ....... [93m[NO][0m
h800-l05: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
h800-l05: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
h800-l05: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
h800-l05: [93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
h800-l05: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
h800-l05: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: --------------------------------------------------
h800-l05: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: --------------------------------------------------
h800-l05: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: DeepSpeed general environment info:
h800-l05: torch install path ............... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/torch']
h800-l05: torch version .................... 2.1.0a0+git7bcf7da
h800-l05: deepspeed install path ........... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/deepspeed']
h800-l05: deepspeed info ................... 0.10.3+04a6fedf, 04a6fedf, HEAD
h800-l05: torch cuda version ............... 12.2
h800-l05: torch hip version ................ None
h800-l05: nvcc version ..................... 12.2
h800-l05: deepspeed wheel compiled w. ...... torch 2.1, cuda 12.2
h800-l05: shared memory (/dev/shm) size .... 755.78 GB
h800-l05: DeepSpeed general environment info:
h800-l05: torch install path ............... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/torch']
h800-l05: torch version .................... 2.1.0a0+git7bcf7da
h800-l05: deepspeed install path ........... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/deepspeed']
h800-l05: deepspeed info ................... 0.10.3+04a6fedf, 04a6fedf, HEAD
h800-l05: torch cuda version ............... 12.2
h800-l05: torch hip version ................ None
h800-l05: nvcc version ..................... 12.2
h800-l05: deepspeed wheel compiled w. ...... torch 2.1, cuda 12.2
h800-l05: shared memory (/dev/shm) size .... 755.78 GB
h800-l05: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: --------------------------------------------------
h800-l05: **** Git info for Megatron: git_hash=a8434f7 git_branch=main ****
h800-l05: **** Git info for Megatron: git_hash=a8434f7 git_branch=main ****
h800-l05: DeepSpeed general environment info:
h800-l05: torch install path ............... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/torch']
h800-l05: torch version .................... 2.1.0a0+git7bcf7da
h800-l05: deepspeed install path ........... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/deepspeed']
h800-l05: deepspeed info ................... 0.10.3+04a6fedf, 04a6fedf, HEAD
h800-l05: torch cuda version ............... 12.2
h800-l05: torch hip version ................ None
h800-l05: nvcc version ..................... 12.2
h800-l05: deepspeed wheel compiled w. ...... torch 2.1, cuda 12.2
h800-l05: shared memory (/dev/shm) size .... 755.78 GB
h800-l03: [2024-05-08 16:03:52,702] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
h800-l05: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
h800-l05: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
h800-l05: async_io ............... [93m[NO][0m ....... [93m[NO][0m
h800-l05: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
h800-l05: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
h800-l05: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
h800-l05: [93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
h800-l05: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
h800-l03: [2024-05-08 16:03:52,706] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
h800-l05: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
h800-l05: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
h800-l05: **** Git info for Megatron: git_hash=a8434f7 git_branch=main ****
h800-l05: **** Git info for Megatron: git_hash=a8434f7 git_branch=main ****
h800-l05: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: using world size: 32, data-parallel-size: 4, sequence-parallel size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 8 
h800-l05: WARNING: overriding default arguments for tokenizer_type:GPT2BPETokenizer                        with tokenizer_type:QwenTokenizer
h800-l05: using torch.float16 for parameters ...
h800-l05: ------------------------ arguments ------------------------
h800-l05:   accumulate_allreduce_grads_in_fp32 .............. False
h800-l05:   adam_beta1 ...................................... 0.9
h800-l05:   adam_beta2 ...................................... 0.95
h800-l05:   adam_eps ........................................ 1e-08
h800-l05:   add_bias_linear ................................. True
h800-l05:   add_position_embedding .......................... True
h800-l05:   adlr_autoresume ................................. False
h800-l05:   adlr_autoresume_interval ........................ 1000
h800-l05:   aml_data_download_path .......................... None
h800-l05:   apply_layernorm_1p .............................. False
h800-l05:   apply_query_key_layer_scaling ................... True
h800-l05:   apply_residual_connection_post_layernorm ........ False
h800-l05:   async_tensor_model_parallel_allreduce ........... False
h800-l05:   attention_dropout ............................... 0.1
h800-l05:   attention_softmax_in_fp32 ....................... False
h800-l05:   barrier_with_L1_time ............................ True
h800-l05:   bert_binary_head ................................ True
h800-l05:   bert_embedder_type .............................. megatron
h800-l05:   bert_load ....................................... None
h800-l05:   bf16 ............................................ False
h800-l05:   bias_dropout_fusion ............................. True
h800-l05:   bias_gelu_fusion ................................ True
h800-l05:   biencoder_projection_dim ........................ 0
h800-l05:   biencoder_shared_query_context_model ............ False
h800-l05:   block_data_path ................................. None
h800-l05:   checkpoint_activations .......................... True
h800-l05:   checkpoint_in_cpu ............................... False
h800-l05:   checkpoint_num_layers ........................... 1
h800-l05:   classes_fraction ................................ 1.0
h800-l05:   clip_grad ....................................... 1.0
h800-l05:   compression_training ............................ False
h800-l05:   consumed_train_samples .......................... 0
h800-l05:   consumed_train_tokens ........................... 0
h800-l05:   consumed_valid_samples .......................... 0
h800-l05:   contigious_checkpointing ........................ False
h800-l05:   cpu_optimizer ................................... True
h800-l05:   cpu_torch_adam .................................. False
h800-l05:   create_moe_param_group .......................... False
h800-l05:   curriculum_learning_legacy ...................... False
h800-l05:   data_cache_path ................................. None
h800-l05:   data_efficiency_curriculum_learning ............. False
h800-l05:   data_impl ....................................... infer
h800-l05:   data_parallel_random_init ....................... False
h800-l05:   data_parallel_size .............................. 4
h800-l05:   data_path ....................................... /share/train_data
h800-l05:   data_per_class_fraction ......................... 1.0
h800-l05:   data_sharding ................................... True
h800-l05:   dataloader_type ................................. single
h800-l05:   DDP_impl ........................................ local
h800-l05:   decoder_num_layers .............................. None
h800-l05:   decoder_seq_length .............................. None
h800-l05:   deepscale ....................................... False
h800-l05:   deepscale_config ................................ None
h800-l05:   deepspeed ....................................... True
h800-l05:   deepspeed_activation_checkpointing .............. True
h800-l05:   deepspeed_config ................................ /data/nvme3/Megatron-DeepSpeed-patch/ds_config.json
h800-l05:   deepspeed_mpi ................................... False
h800-l05:   dino_bottleneck_size ............................ 256
h800-l05:   dino_freeze_last_layer .......................... 1
h800-l05:   dino_head_hidden_size ........................... 2048
h800-l05:   dino_local_crops_number ......................... 10
h800-l05:   dino_local_img_size ............................. 96
h800-l05:   dino_norm_last_layer ............................ False
h800-l05:   dino_teacher_temp ............................... 0.07
h800-l05:   dino_warmup_teacher_temp ........................ 0.04
h800-l05:   dino_warmup_teacher_temp_epochs ................. 30
h800-l05:   distribute_checkpointed_activations ............. False
h800-l05:   distribute_saved_activations .................... False
h800-l05:   distributed_backend ............................. nccl
h800-l05:   distributed_timeout_minutes ..................... 10
h800-l05:   ds_fused_adam ................................... False
h800-l05:   ds_inference .................................... False
h800-l05:   ds_pipeline_enabled ............................. True
h800-l05:   ds_sequence_parallel_size ....................... 1
h800-l05:   embedding_path .................................. None
h800-l05:   embedding_weights_in_fp32 ....................... False
h800-l05:   empty_unused_memory_level ....................... 0
h800-l05:   enable_expert_tensor_parallelism ................ False
h800-l05:   encoder_num_layers .............................. 80
h800-l05:   encoder_seq_length .............................. 8192
h800-l05:   end_weight_decay ................................ 0.1
h800-l05:   eod_mask_loss ................................... False
h800-l05:   eval_interval ................................... 1000000
h800-l05:   eval_iters ...................................... 40
h800-l05:   evidence_data_path .............................. None
h800-l05:   exit_duration_in_mins ........................... None
h800-l05:   exit_interval ................................... None
h800-l05:   exit_on_missing_checkpoint ...................... False
h800-l05:   exit_signal_handler ............................. False
h800-l05:   expert_interval ................................. 2
h800-l05:   ffn_hidden_size ................................. 32768
h800-l05:   finetune ........................................ False
h800-l05:   force_ds_sequence_parallel ...................... False
h800-l05:   fp16 ............................................ True
h800-l05:   fp16_lm_cross_entropy ........................... False
h800-l05:   fp32_residual_connection ........................ False
h800-l05:   fp8_amax_compute_algo ........................... most_recent
h800-l05:   fp8_amax_history_len ............................ 1
h800-l05:   fp8_e4m3 ........................................ False
h800-l05:   fp8_hybrid ...................................... False
h800-l05:   fp8_interval .................................... 1
h800-l05:   fp8_margin ...................................... 0
h800-l05:   fp8_wgrad ....................................... True
h800-l05:   global_batch_size ............................... 128
h800-l05:   gradient_accumulation_fusion .................... True
h800-l05:   head_lr_mult .................................... 1.0
h800-l05:   hidden_dropout .................................. 0.1
h800-l05:   hidden_size ..................................... 8192
h800-l05:   hidden_size_teacher ............................. None
h800-l05:   hysteresis ...................................... 2
h800-l05:   ict_head_size ................................... None
h800-l05:   ict_load ........................................ None
h800-l05:   img_h ........................................... 224
h800-l05:   img_w ........................................... 224
h800-l05:   indexer_batch_size .............................. 128
h800-l05:   indexer_log_interval ............................ 1000
h800-l05:   inference ....................................... False
h800-l05:   inference_batch_times_seqlen_threshold .......... 512
h800-l05:   init_method_std ................................. 0.006
h800-l05:   init_method_xavier_uniform ...................... False
h800-l05:   initial_loss_scale .............................. 4294967296
h800-l05:   iter_per_epoch .................................. 1250
h800-l05:   kd .............................................. False
h800-l05:   kd_alpha_ce ..................................... 1
h800-l05:   kd_beta_ce ...................................... 1
h800-l05:   kd_temp ......................................... 1.0
h800-l05:   kv_channels ..................................... 128
h800-l05:   layernorm_epsilon ............................... 1e-05
h800-l05:   lazy_mpu_init ................................... None
h800-l05:   load ............................................ ./configs
h800-l05:   load_tag ........................................ None
h800-l05:   load_teacher .................................... None
h800-l05:   local_rank ...................................... 0
h800-l05:   log_batch_size_to_tensorboard ................... False
h800-l05:   log_interval .................................... 1
h800-l05:   log_learning_rate_to_tensorboard ................ True
h800-l05:   log_loss_scale_to_tensorboard ................... True
h800-l05:   log_memory_to_tensorboard ....................... False
h800-l05:   log_num_zeros_in_grad ........................... False
h800-l05:   log_optimizer_states_to_tensorboard ............. False
h800-l05:   log_params_norm ................................. False
h800-l05:   log_timers_to_tensorboard ....................... False
h800-l05:   log_validation_ppl_to_tensorboard ............... False
h800-l05:   log_world_size_to_tensorboard ................... False
h800-l05:   loss_scale ...................................... 12.0
h800-l05:   loss_scale_window ............................... 1000
h800-l05:   lr .............................................. 0.0002
h800-l05:   lr_decay_iters .................................. 10000
h800-l05:   lr_decay_samples ................................ None
h800-l05:   lr_decay_style .................................. cosine
h800-l05:   lr_decay_tokens ................................. None
h800-l05:   lr_warmup_fraction .............................. None
h800-l05:   lr_warmup_iters ................................. 1000
h800-l05:   lr_warmup_samples ............................... 0
h800-l05:   lr_warmup_tokens ................................ None
h800-l05:   make_vocab_size_divisible_by .................... 128
h800-l05:   mask_factor ..................................... 1.0
h800-l05:   mask_prob ....................................... 0.15
h800-l05:   mask_type ....................................... random
h800-l05:   masked_softmax_fusion ........................... True
h800-l05:   max_position_embeddings ......................... 8192
h800-l05:   max_tokens_to_oom ............................... 12000
h800-l05:   mem_efficient_ln ................................ True
h800-l05:   memory_centric_tiled_linear ..................... False
h800-l05:   merge_file ...................................... None
h800-l05:   micro_batch_size ................................ 1
h800-l05:   min_loss_scale .................................. 1.0
h800-l05:   min_lr .......................................... 2e-05
h800-l05:   mlp_type ........................................ standard
h800-l05:   mmap_warmup ..................................... False
h800-l05:   moe_eval_capacity_factor ........................ 1.0
h800-l05:   moe_expert_parallel_size ........................ 1
h800-l05:   moe_loss_coeff .................................. 0.1
h800-l05:   moe_min_capacity ................................ 4
h800-l05:   moe_token_dropping .............................. True
h800-l05:   moe_top2_2nd_expert_sampling .................... True
h800-l05:   moe_train_capacity_factor ....................... 1.0
h800-l05:   mos ............................................. False
h800-l05:   no_load_lr_state ................................ False
h800-l05:   no_load_optim ................................... None
h800-l05:   no_load_rng ..................................... None
h800-l05:   no_persist_layer_norm ........................... False
h800-l05:   no_pipeline_parallel ............................ False
h800-l05:   no_save_optim ................................... None
h800-l05:   no_save_rng ..................................... None
h800-l05:   normalization ................................... layernorm
h800-l05:   num_attention_heads ............................. 64
h800-l05:   num_attention_heads_teacher ..................... None
h800-l05:   num_channels .................................... 3
h800-l05:   num_classes ..................................... 1000
h800-l05:   num_experts ..................................... [1]
h800-l05:   num_experts_switch .............................. None
h800-l05:   num_experts_teacher ............................. [1]
h800-l05:   num_key_value_heads ............................. 64
h800-l05:   num_layers ...................................... 80
h800-l05:   num_layers_per_virtual_pipeline_stage ........... None
h800-l05:   num_layers_teacher .............................. None
h800-l05:   num_workers ..................................... 2
h800-l05:   onnx_safe ....................................... None
h800-l05:   openai_gelu ..................................... False
h800-l05:   optimizer ....................................... adam
h800-l05:   output_bert_embeddings .......................... False
h800-l05:   overlap_p2p_comm ................................ False
h800-l05:   override_opt_param_scheduler .................... False
h800-l05:   params_dtype .................................... torch.float16
h800-l05:   partition_activations ........................... False
h800-l05:   patch_dim ....................................... 16
h800-l05:   perform_initialization .......................... True
h800-l05:   pipeline_model_parallel_size .................... 8
h800-l05:   pipeline_model_parallel_split_rank .............. None
h800-l05:   profile_backward ................................ False
h800-l05:   query_in_block_prob ............................. 0.1
h800-l05:   rampup_batch_size ............................... None
h800-l05:   random_ltd ...................................... False
h800-l05:   rank ............................................ 0
h800-l05:   recompute_granularity ........................... None
h800-l05:   recompute_method ................................ None
h800-l05:   recompute_num_layers ............................ 1
h800-l05:   remote_device ................................... none
h800-l05:   repeated_dataloader ............................. False
h800-l05:   reset_attention_mask ............................ False
h800-l05:   reset_iteration ................................. False
h800-l05:   reset_position_ids .............................. False
h800-l05:   retriever_report_topk_accuracies ................ []
h800-l05:   retriever_score_scaling ......................... False
h800-l05:   retriever_seq_length ............................ 256
h800-l05:   retro_add_retriever ............................. False
h800-l05:   retro_cyclic_train_iters ........................ None
h800-l05:   retro_encoder_attention_dropout ................. 0.1
h800-l05:   retro_encoder_hidden_dropout .................... 0.1
h800-l05:   retro_encoder_layers ............................ 2
h800-l05:   retro_num_neighbors ............................. 2
h800-l05:   retro_num_retrieved_chunks ...................... 2
h800-l05:   retro_return_doc_ids ............................ False
h800-l05:   retro_workdir ................................... None
h800-l05:   return_data_index ............................... False
h800-l05:   rope_theta ...................................... 10000
h800-l05:   rotary_percent .................................. 1.0
h800-l05:   sample_rate ..................................... 1.0
h800-l05:   save ............................................ ./configs
h800-l05:   save_interval ................................... 100
h800-l05:   scatter_gather_tensors_in_pipeline .............. True
h800-l05:   scattered_embeddings ............................ False
h800-l05:   seed ............................................ 1234
h800-l05:   seq_length ...................................... 8192
h800-l05:   sequence_parallel ............................... False
h800-l05:   sgd_momentum .................................... 0.9
h800-l05:   short_seq_prob .................................. 0.1
h800-l05:   skip_train ...................................... False
h800-l05:   split ........................................... 800,100,100
h800-l05:   split_transformers .............................. False
h800-l05:   squared_relu .................................... False
h800-l05:   standalone_embedding_stage ...................... False
h800-l05:   start_weight_decay .............................. 0.1
h800-l05:   swiglu .......................................... False
h800-l05:   swin_backbone_type .............................. tiny
h800-l05:   synchronize_each_layer .......................... False
h800-l05:   tensor_model_parallel_size ...................... 1
h800-l05:   tensorboard_dir ................................. None
h800-l05:   tensorboard_log_interval ........................ 1
h800-l05:   tensorboard_queue_size .......................... 1000
h800-l05:   test_data_path .................................. None
h800-l05:   tile_factor ..................................... 1
h800-l05:   timing_log_level ................................ 0
h800-l05:   timing_log_option ............................... minmax
h800-l05:   titles_data_path ................................ None
h800-l05:   tokenizer_model ................................. None
h800-l05:   tokenizer_type .................................. QwenTokenizer
h800-l05:   topk ............................................ 1
h800-l05:   train_data_exact_num_epochs ..................... None
h800-l05:   train_data_path ................................. None
h800-l05:   train_desc_path ................................. None
h800-l05:   train_doc_idx_path .............................. None
h800-l05:   train_idx_path .................................. None
h800-l05:   train_iters ..................................... 200000
h800-l05:   train_sample_idx_path ........................... None
h800-l05:   train_samples ................................... None
h800-l05:   train_shuffle_idx_path .......................... None
h800-l05:   train_tokens .................................... None
h800-l05:   transformer_impl ................................ local
h800-l05:   transformer_pipeline_model_parallel_size ........ 8
h800-l05:   universal_checkpoint ............................ False
h800-l05:   untie_embeddings_and_output_weights ............. False
h800-l05:   use_checkpoint_args ............................. False
h800-l05:   use_checkpoint_opt_param_scheduler .............. False
h800-l05:   use_contiguous_buffers_in_local_ddp ............. True
h800-l05:   use_cpu_initialization .......................... None
h800-l05: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
h800-l05:   use_dataset_only ................................ False
h800-l05:   use_distributed_optimizer ....................... False
h800-l05:   use_flash_attn .................................. True
h800-l05:   use_flash_attn_builder .......................... False
h800-l05:   use_flash_attn_triton ........................... False
h800-l05:   use_flash_attn_v1 ............................... False
h800-l05:   use_flash_attn_v2 ............................... True
h800-l05:   use_one_sent_docs ............................... False
h800-l05:   use_pin_memory .................................. False
h800-l05:   use_ring_exchange_p2p ........................... False
h800-l05:   use_rotary_position_embeddings .................. False
h800-l05:   use_tutel ....................................... False
h800-l05:   valid_data_path ................................. None
h800-l05:   variable_seq_lengths ............................ False
h800-l05:   virtual_pipeline_model_parallel_size ............ None
h800-l05:   vision_backbone_type ............................ vit
h800-l05:   vision_pretraining .............................. False
h800-l05:   vision_pretraining_type ......................... classify
h800-l05:   vocab_extra_ids ................................. 0
h800-l05:   vocab_file ...................................... ./ch_tokenizer_data/vocab.txt
h800-l05:   vocab_size ...................................... None
h800-l05:   weight_decay .................................... 0.1
h800-l05:   weight_decay_incr_style ......................... constant
h800-l05:   world_size ...................................... 32
h800-l05:   zero_allgather_bucket_size ...................... 0.0
h800-l05:   zero_contigious_gradients ....................... False
h800-l05:   zero_reduce_bucket_size ......................... 0.0
h800-l05:   zero_reduce_scatter ............................. False
h800-l05:   zero_stage ...................................... 0
h800-l05: -------------------- end of arguments ---------------------
h800-l05: setting number of micro-batches to constant 32
h800-l05: > building QwenTokenizer tokenizer ...
h800-l05: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
h800-l05: --------------------------------------------------
h800-l05: **** Git info for Megatron: git_hash=a8434f7 git_branch=main ****
h800-l05:  > padded vocab (size: 64001) with 127 dummy tokens (new size: 64128)
h800-l05: > initializing torch distributed ...
h800-l05: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
h800-l05: [2024-05-08 16:03:52,702] [INFO] [comm.py:637:init_distributed] cdb=None
h800-l05: [2024-05-08 16:03:52,702] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
h800-l05: DeepSpeed general environment info:
h800-l05: torch install path ............... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/torch']
h800-l05: torch version .................... 2.1.0a0+git7bcf7da
h800-l05: deepspeed install path ........... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/deepspeed']
h800-l05: deepspeed info ................... 0.10.3+04a6fedf, 04a6fedf, HEAD
h800-l05: torch cuda version ............... 12.2
h800-l05: torch hip version ................ None
h800-l05: nvcc version ..................... 12.2
h800-l05: deepspeed wheel compiled w. ...... torch 2.1, cuda 12.2
h800-l05: shared memory (/dev/shm) size .... 755.78 GB
h800-l05: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
h800-l03: [2024-05-08 16:03:52,782] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
h800-l03: [2024-05-08 16:03:52,782] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
h800-l03: [2024-05-08 16:03:52,784] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
h800-l03: [2024-05-08 16:03:52,791] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
h800-l05: **** Git info for Megatron: git_hash=a8434f7 git_branch=main ****
h800-l03: [2024-05-08 16:03:52,808] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
h800-l05: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
h800-l03: [2024-05-08 16:03:52,834] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
h800-l05: [2024-05-08 16:03:53,283] [INFO] [comm.py:637:init_distributed] cdb=None
h800-l05: [2024-05-08 16:03:53,294] [INFO] [comm.py:637:init_distributed] cdb=None
h800-l05: [2024-05-08 16:03:53,319] [INFO] [comm.py:637:init_distributed] cdb=None
h800-l05: [2024-05-08 16:03:53,327] [INFO] [comm.py:637:init_distributed] cdb=None
h800-l05: [2024-05-08 16:03:53,331] [INFO] [comm.py:637:init_distributed] cdb=None
h800-l07: /data/nvme3/Megatron-DeepSpeed/megatron/model/module.py:13: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /data/nvme5/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
h800-l07:   _FLOAT_TYPES = [get_accelerator().FloatTensor(0).dtype]
h800-l07: --------------------------------------------------
h800-l07: DeepSpeed C++/CUDA extension op report
h800-l07: --------------------------------------------------
h800-l07: NOTE: Ops not installed will be just-in-time (JIT) compiled at
h800-l07:       runtime if needed. Op compatibility means that your system
h800-l07:       meet the required dependencies to JIT install the op.
h800-l07: --------------------------------------------------
h800-l07: JIT compiled ops requires ninja
h800-l07: ninja .................. [92m[OKAY][0m
h800-l07: --------------------------------------------------
h800-l07: op name ................ installed .. compatible
h800-l07: --------------------------------------------------
h800-l07: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
h800-l07: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
h800-l07: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
h800-l07: [93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
h800-l07: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
h800-l07: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: /data/nvme3/Megatron-DeepSpeed/megatron/model/module.py:13: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /data/nvme5/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
h800-l09:   _FLOAT_TYPES = [get_accelerator().FloatTensor(0).dtype]
h800-l07: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: --------------------------------------------------
h800-l09: --------------------------------------------------
h800-l09: DeepSpeed C++/CUDA extension op report
h800-l09: --------------------------------------------------
h800-l09: NOTE: Ops not installed will be just-in-time (JIT) compiled at
h800-l09:       runtime if needed. Op compatibility means that your system
h800-l09:       meet the required dependencies to JIT install the op.
h800-l09: --------------------------------------------------
h800-l09: JIT compiled ops requires ninja
h800-l07: DeepSpeed general environment info:
h800-l07: torch install path ............... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/torch']
h800-l07: torch version .................... 2.1.0a0+git7bcf7da
h800-l07: deepspeed install path ........... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/deepspeed']
h800-l07: deepspeed info ................... 0.10.3+04a6fedf, 04a6fedf, HEAD
h800-l07: torch cuda version ............... 12.2
h800-l07: torch hip version ................ None
h800-l07: nvcc version ..................... 12.2
h800-l07: deepspeed wheel compiled w. ...... torch 2.1, cuda 12.2
h800-l07: shared memory (/dev/shm) size .... 755.78 GB
h800-l09: ninja .................. [92m[OKAY][0m
h800-l09: --------------------------------------------------
h800-l09: op name ................ installed .. compatible
h800-l09: --------------------------------------------------
h800-l07: fatal: detected dubious ownership in repository at '/data/nvme3/Megatron-DeepSpeed-patch'
h800-l07: To add an exception for this directory, call:
h800-l07: 
h800-l07: 	git config --global --add safe.directory /data/nvme3/Megatron-DeepSpeed-patch
h800-l07: **** Git info for Megatron: git_hash=unknown git_branch=unknown ****
h800-l07: [2024-05-08 16:03:54,539] [INFO] [comm.py:637:init_distributed] cdb=None
h800-l07: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
h800-l09: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
h800-l09: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
h800-l09: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
h800-l09: [93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
h800-l09: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
h800-l09: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: --------------------------------------------------
h800-l09: DeepSpeed general environment info:
h800-l09: torch install path ............... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/torch']
h800-l09: torch version .................... 2.1.0a0+git7bcf7da
h800-l09: deepspeed install path ........... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/deepspeed']
h800-l09: deepspeed info ................... 0.10.3+04a6fedf, 04a6fedf, HEAD
h800-l09: torch cuda version ............... 12.2
h800-l09: torch hip version ................ None
h800-l09: nvcc version ..................... 12.2
h800-l09: deepspeed wheel compiled w. ...... torch 2.1, cuda 12.2
h800-l09: shared memory (/dev/shm) size .... 755.78 GB
h800-l09: fatal: detected dubious ownership in repository at '/data/nvme3/Megatron-DeepSpeed-patch'
h800-l09: To add an exception for this directory, call:
h800-l09: 
h800-l09: 	git config --global --add safe.directory /data/nvme3/Megatron-DeepSpeed-patch
h800-l09: **** Git info for Megatron: git_hash=unknown git_branch=unknown ****
h800-l07: /data/nvme3/Megatron-DeepSpeed/megatron/model/module.py:13: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /data/nvme5/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
h800-l07:   _FLOAT_TYPES = [get_accelerator().FloatTensor(0).dtype]
h800-l09: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
h800-l07: --------------------------------------------------
h800-l07: DeepSpeed C++/CUDA extension op report
h800-l07: --------------------------------------------------
h800-l07: NOTE: Ops not installed will be just-in-time (JIT) compiled at
h800-l07:       runtime if needed. Op compatibility means that your system
h800-l07:       meet the required dependencies to JIT install the op.
h800-l07: --------------------------------------------------
h800-l07: JIT compiled ops requires ninja
h800-l07: ninja .................. [92m[OKAY][0m
h800-l07: --------------------------------------------------
h800-l07: op name ................ installed .. compatible
h800-l07: --------------------------------------------------
h800-l07: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
h800-l07: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
h800-l07: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
h800-l07: [93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
h800-l07: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
h800-l07: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: --------------------------------------------------
h800-l07: DeepSpeed general environment info:
h800-l07: torch install path ............... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/torch']
h800-l07: torch version .................... 2.1.0a0+git7bcf7da
h800-l07: deepspeed install path ........... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/deepspeed']
h800-l07: deepspeed info ................... 0.10.3+04a6fedf, 04a6fedf, HEAD
h800-l07: torch cuda version ............... 12.2
h800-l07: torch hip version ................ None
h800-l07: nvcc version ..................... 12.2
h800-l07: deepspeed wheel compiled w. ...... torch 2.1, cuda 12.2
h800-l07: shared memory (/dev/shm) size .... 755.78 GB
h800-l07: fatal: detected dubious ownership in repository at '/data/nvme3/Megatron-DeepSpeed-patch'
h800-l07: To add an exception for this directory, call:
h800-l07: 
h800-l07: 	git config --global --add safe.directory /data/nvme3/Megatron-DeepSpeed-patch
h800-l07: **** Git info for Megatron: git_hash=unknown git_branch=unknown ****
h800-l07: /data/nvme3/Megatron-DeepSpeed/megatron/model/module.py:13: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /data/nvme5/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
h800-l07:   _FLOAT_TYPES = [get_accelerator().FloatTensor(0).dtype]
h800-l07: --------------------------------------------------
h800-l07: DeepSpeed C++/CUDA extension op report
h800-l07: --------------------------------------------------
h800-l07: NOTE: Ops not installed will be just-in-time (JIT) compiled at
h800-l07:       runtime if needed. Op compatibility means that your system
h800-l07:       meet the required dependencies to JIT install the op.
h800-l07: --------------------------------------------------
h800-l07: JIT compiled ops requires ninja
h800-l07: ninja .................. [92m[OKAY][0m
h800-l07: --------------------------------------------------
h800-l07: op name ................ installed .. compatible
h800-l07: --------------------------------------------------
h800-l07: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
h800-l07: /data/nvme3/Megatron-DeepSpeed/megatron/model/module.py:13: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /data/nvme5/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
h800-l07:   _FLOAT_TYPES = [get_accelerator().FloatTensor(0).dtype]
h800-l07: --------------------------------------------------
h800-l07: DeepSpeed C++/CUDA extension op report
h800-l07: --------------------------------------------------
h800-l07: NOTE: Ops not installed will be just-in-time (JIT) compiled at
h800-l07:       runtime if needed. Op compatibility means that your system
h800-l07:       meet the required dependencies to JIT install the op.
h800-l07: --------------------------------------------------
h800-l07: JIT compiled ops requires ninja
h800-l07: ninja .................. [92m[OKAY][0m
h800-l07: --------------------------------------------------
h800-l07: op name ................ installed .. compatible
h800-l07: --------------------------------------------------
h800-l09: /data/nvme3/Megatron-DeepSpeed/megatron/model/module.py:13: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /data/nvme5/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
h800-l09:   _FLOAT_TYPES = [get_accelerator().FloatTensor(0).dtype]
h800-l09: /data/nvme3/Megatron-DeepSpeed/megatron/model/module.py:13: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /data/nvme5/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
h800-l09:   _FLOAT_TYPES = [get_accelerator().FloatTensor(0).dtype]
h800-l09: /data/nvme3/Megatron-DeepSpeed/megatron/model/module.py:13: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /data/nvme5/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
h800-l09:   _FLOAT_TYPES = [get_accelerator().FloatTensor(0).dtype]
h800-l09: /data/nvme3/Megatron-DeepSpeed/megatron/model/module.py:13: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /data/nvme5/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
h800-l09:   _FLOAT_TYPES = [get_accelerator().FloatTensor(0).dtype]
h800-l09: /data/nvme3/Megatron-DeepSpeed/megatron/model/module.py:13: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /data/nvme5/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
h800-l09:   _FLOAT_TYPES = [get_accelerator().FloatTensor(0).dtype]
h800-l09: /data/nvme3/Megatron-DeepSpeed/megatron/model/module.py:13: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /data/nvme5/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
h800-l09:   _FLOAT_TYPES = [get_accelerator().FloatTensor(0).dtype]
h800-l09: --------------------------------------------------
h800-l09: DeepSpeed C++/CUDA extension op report
h800-l09: --------------------------------------------------
h800-l09: NOTE: Ops not installed will be just-in-time (JIT) compiled at
h800-l09:       runtime if needed. Op compatibility means that your system
h800-l09:       meet the required dependencies to JIT install the op.
h800-l09: --------------------------------------------------
h800-l09: JIT compiled ops requires ninja
h800-l09: ninja .................. [92m[OKAY][0m
h800-l09: --------------------------------------------------
h800-l09: op name ................ installed .. compatible
h800-l09: --------------------------------------------------
h800-l09: --------------------------------------------------
h800-l09: DeepSpeed C++/CUDA extension op report
h800-l09: --------------------------------------------------
h800-l09: NOTE: Ops not installed will be just-in-time (JIT) compiled at
h800-l09:       runtime if needed. Op compatibility means that your system
h800-l09:       meet the required dependencies to JIT install the op.
h800-l09: --------------------------------------------------
h800-l09: JIT compiled ops requires ninja
h800-l09: ninja .................. [92m[OKAY][0m
h800-l09: --------------------------------------------------
h800-l09: op name ................ installed .. compatible
h800-l09: --------------------------------------------------
h800-l09: --------------------------------------------------
h800-l09: DeepSpeed C++/CUDA extension op report
h800-l09: --------------------------------------------------
h800-l09: NOTE: Ops not installed will be just-in-time (JIT) compiled at
h800-l09:       runtime if needed. Op compatibility means that your system
h800-l09:       meet the required dependencies to JIT install the op.
h800-l09: --------------------------------------------------
h800-l09: JIT compiled ops requires ninja
h800-l09: ninja .................. [92m[OKAY][0m
h800-l09: --------------------------------------------------
h800-l09: op name ................ installed .. compatible
h800-l09: --------------------------------------------------
h800-l09: /data/nvme3/Megatron-DeepSpeed/megatron/model/module.py:13: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /data/nvme5/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
h800-l09:   _FLOAT_TYPES = [get_accelerator().FloatTensor(0).dtype]
h800-l07: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
h800-l07: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
h800-l07: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
h800-l07: [93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
h800-l07: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
h800-l09: --------------------------------------------------
h800-l09: DeepSpeed C++/CUDA extension op report
h800-l09: --------------------------------------------------
h800-l09: NOTE: Ops not installed will be just-in-time (JIT) compiled at
h800-l09:       runtime if needed. Op compatibility means that your system
h800-l09:       meet the required dependencies to JIT install the op.
h800-l09: --------------------------------------------------
h800-l09: JIT compiled ops requires ninja
h800-l09: ninja .................. [92m[OKAY][0m
h800-l09: --------------------------------------------------
h800-l09: op name ................ installed .. compatible
h800-l09: --------------------------------------------------
h800-l09: --------------------------------------------------
h800-l09: DeepSpeed C++/CUDA extension op report
h800-l09: --------------------------------------------------
h800-l09: NOTE: Ops not installed will be just-in-time (JIT) compiled at
h800-l09:       runtime if needed. Op compatibility means that your system
h800-l09:       meet the required dependencies to JIT install the op.
h800-l09: --------------------------------------------------
h800-l09: JIT compiled ops requires ninja
h800-l09: ninja .................. [92m[OKAY][0m
h800-l09: --------------------------------------------------
h800-l09: op name ................ installed .. compatible
h800-l09: --------------------------------------------------
h800-l09: [2024-05-08 16:03:54,917] [INFO] [comm.py:637:init_distributed] cdb=None
h800-l09: --------------------------------------------------
h800-l09: DeepSpeed C++/CUDA extension op report
h800-l09: --------------------------------------------------
h800-l09: NOTE: Ops not installed will be just-in-time (JIT) compiled at
h800-l09:       runtime if needed. Op compatibility means that your system
h800-l09:       meet the required dependencies to JIT install the op.
h800-l09: --------------------------------------------------
h800-l09: JIT compiled ops requires ninja
h800-l09: ninja .................. [92m[OKAY][0m
h800-l09: --------------------------------------------------
h800-l09: op name ................ installed .. compatible
h800-l09: --------------------------------------------------
h800-l07: /data/nvme3/Megatron-DeepSpeed/megatron/model/module.py:13: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /data/nvme5/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
h800-l07:   _FLOAT_TYPES = [get_accelerator().FloatTensor(0).dtype]
h800-l07: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: /data/nvme3/Megatron-DeepSpeed/megatron/model/module.py:13: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /data/nvme5/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
h800-l07:   _FLOAT_TYPES = [get_accelerator().FloatTensor(0).dtype]
h800-l09: --------------------------------------------------
h800-l09: DeepSpeed C++/CUDA extension op report
h800-l09: --------------------------------------------------
h800-l09: NOTE: Ops not installed will be just-in-time (JIT) compiled at
h800-l09:       runtime if needed. Op compatibility means that your system
h800-l09:       meet the required dependencies to JIT install the op.
h800-l09: --------------------------------------------------
h800-l09: JIT compiled ops requires ninja
h800-l09: ninja .................. [92m[OKAY][0m
h800-l09: --------------------------------------------------
h800-l09: op name ................ installed .. compatible
h800-l09: --------------------------------------------------
h800-l07: /data/nvme3/Megatron-DeepSpeed/megatron/model/module.py:13: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /data/nvme5/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
h800-l07:   _FLOAT_TYPES = [get_accelerator().FloatTensor(0).dtype]
h800-l07: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: --------------------------------------------------
h800-l07: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
h800-l07: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
h800-l07: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
h800-l07: [93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
h800-l07: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
h800-l07: --------------------------------------------------
h800-l07: DeepSpeed C++/CUDA extension op report
h800-l07: --------------------------------------------------
h800-l07: NOTE: Ops not installed will be just-in-time (JIT) compiled at
h800-l07:       runtime if needed. Op compatibility means that your system
h800-l07:       meet the required dependencies to JIT install the op.
h800-l07: --------------------------------------------------
h800-l07: JIT compiled ops requires ninja
h800-l07: ninja .................. [92m[OKAY][0m
h800-l07: --------------------------------------------------
h800-l07: op name ................ installed .. compatible
h800-l07: --------------------------------------------------
h800-l07: DeepSpeed general environment info:
h800-l07: torch install path ............... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/torch']
h800-l07: torch version .................... 2.1.0a0+git7bcf7da
h800-l07: deepspeed install path ........... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/deepspeed']
h800-l07: deepspeed info ................... 0.10.3+04a6fedf, 04a6fedf, HEAD
h800-l07: torch cuda version ............... 12.2
h800-l07: torch hip version ................ None
h800-l07: nvcc version ..................... 12.2
h800-l07: deepspeed wheel compiled w. ...... torch 2.1, cuda 12.2
h800-l07: shared memory (/dev/shm) size .... 755.78 GB
h800-l07: --------------------------------------------------
h800-l07: DeepSpeed C++/CUDA extension op report
h800-l07: --------------------------------------------------
h800-l07: NOTE: Ops not installed will be just-in-time (JIT) compiled at
h800-l07:       runtime if needed. Op compatibility means that your system
h800-l07:       meet the required dependencies to JIT install the op.
h800-l07: --------------------------------------------------
h800-l07: JIT compiled ops requires ninja
h800-l07: ninja .................. [92m[OKAY][0m
h800-l07: --------------------------------------------------
h800-l07: op name ................ installed .. compatible
h800-l07: --------------------------------------------------
h800-l07: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: --------------------------------------------------
h800-l07: DeepSpeed C++/CUDA extension op report
h800-l07: --------------------------------------------------
h800-l07: NOTE: Ops not installed will be just-in-time (JIT) compiled at
h800-l07:       runtime if needed. Op compatibility means that your system
h800-l07:       meet the required dependencies to JIT install the op.
h800-l07: --------------------------------------------------
h800-l07: JIT compiled ops requires ninja
h800-l07: ninja .................. [92m[OKAY][0m
h800-l07: --------------------------------------------------
h800-l07: op name ................ installed .. compatible
h800-l07: --------------------------------------------------
h800-l07: /data/nvme3/Megatron-DeepSpeed/megatron/model/module.py:13: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /data/nvme5/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
h800-l07:   _FLOAT_TYPES = [get_accelerator().FloatTensor(0).dtype]
h800-l07: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: --------------------------------------------------
h800-l07: fatal: detected dubious ownership in repository at '/data/nvme3/Megatron-DeepSpeed-patch'
h800-l07: To add an exception for this directory, call:
h800-l07: 
h800-l07: 	git config --global --add safe.directory /data/nvme3/Megatron-DeepSpeed-patch
h800-l07: **** Git info for Megatron: git_hash=unknown git_branch=unknown ****
h800-l09: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
h800-l09: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
h800-l09: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
h800-l09: [93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
h800-l09: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
h800-l09: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
h800-l09: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
h800-l09: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
h800-l09: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
h800-l09: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
h800-l09: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: random_ltd ............. [93m[NO][0m [93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention.......
h800-l09:  [92m[OKAY][0m
h800-l09: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
h800-l09: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
h800-l09: [93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
h800-l09: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
h800-l07: DeepSpeed general environment info:
h800-l07: torch install path ............... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/torch']
h800-l07: torch version .................... 2.1.0a0+git7bcf7da
h800-l07: deepspeed install path ........... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/deepspeed']
h800-l07: deepspeed info ................... 0.10.3+04a6fedf, 04a6fedf, HEAD
h800-l07: torch cuda version ............... 12.2
h800-l07: torch hip version ................ None
h800-l07: nvcc version ..................... 12.2
h800-l07: deepspeed wheel compiled w. ...... torch 2.1, cuda 12.2
h800-l07: shared memory (/dev/shm) size .... 755.78 GB
h800-l09: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
h800-l09: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
h800-l09: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
h800-l09: [93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
h800-l09: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
h800-l09: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
h800-l09: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
h800-l09: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
h800-l09: [93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
h800-l09: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
h800-l07: --------------------------------------------------
h800-l07: DeepSpeed C++/CUDA extension op report
h800-l07: --------------------------------------------------
h800-l07: NOTE: Ops not installed will be just-in-time (JIT) compiled at
h800-l07:       runtime if needed. Op compatibility means that your system
h800-l07:       meet the required dependencies to JIT install the op.
h800-l07: --------------------------------------------------
h800-l07: JIT compiled ops requires ninja
h800-l07: ninja .................. [92m[OKAY][0m
h800-l07: --------------------------------------------------
h800-l07: op name ................ installed .. compatible
h800-l07: --------------------------------------------------
h800-l09: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
h800-l09: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
h800-l09: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
h800-l09: [93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
h800-l09: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
h800-l09: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
h800-l09: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
h800-l09: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
h800-l09: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
h800-l09: [93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
h800-l09: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
h800-l09: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: fatal: detected dubious ownership in repository at '/data/nvme3/Megatron-DeepSpeed-patch'
h800-l07: To add an exception for this directory, call:
h800-l07: 
h800-l07: 	git config --global --add safe.directory /data/nvme3/Megatron-DeepSpeed-patch
h800-l07: **** Git info for Megatron: git_hash=unknown git_branch=unknown ****
h800-l09: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: --------------------------------------------------
h800-l07: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
h800-l07: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
h800-l07: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
h800-l07: [93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
h800-l07: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
h800-l09: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: --------------------------------------------------
h800-l07: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
h800-l07: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
h800-l07: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
h800-l07: [93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
h800-l07: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
h800-l09: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: --------------------------------------------------
h800-l09: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: --------------------------------------------------
h800-l09: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: --------------------------------------------------
h800-l07: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
h800-l07: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
h800-l07: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
h800-l07: [93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
h800-l07: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
h800-l07: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
h800-l09: DeepSpeed general environment info:
h800-l09: torch install path ............... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/torch']
h800-l09: torch version .................... 2.1.0a0+git7bcf7da
h800-l09: deepspeed install path ........... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/deepspeed']
h800-l09: deepspeed info ................... 0.10.3+04a6fedf, 04a6fedf, HEAD
h800-l09: torch cuda version ............... 12.2
h800-l09: torch hip version ................ None
h800-l09: nvcc version ..................... 12.2
h800-l09: deepspeed wheel compiled w. ...... torch 2.1, cuda 12.2
h800-l09: shared memory (/dev/shm) size .... 755.78 GB
h800-l09: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: --------------------------------------------------
h800-l09: DeepSpeed general environment info:
h800-l09: torch install path ............... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/torch']
h800-l09: torch version .................... 2.1.0a0+git7bcf7da
h800-l09: deepspeed install path ........... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/deepspeed']
h800-l09: deepspeed info ................... 0.10.3+04a6fedf, 04a6fedf, HEAD
h800-l09: torch cuda version ............... 12.2
h800-l09: torch hip version ................ None
h800-l09: nvcc version ..................... 12.2
h800-l09: deepspeed wheel compiled w. ...... torch 2.1, cuda 12.2
h800-l09: shared memory (/dev/shm) size .... 755.78 GB
h800-l07: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: --------------------------------------------------
h800-l09: DeepSpeed general environment info:
h800-l09: torch install path ............... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/torch']
h800-l09: torch version .................... 2.1.0a0+git7bcf7da
h800-l09: deepspeed install path ........... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/deepspeed']
h800-l09: deepspeed info ................... 0.10.3+04a6fedf, 04a6fedf, HEAD
h800-l09: torch cuda version ............... 12.2
h800-l09: torch hip version ................ None
h800-l09: nvcc version ..................... 12.2
h800-l09: deepspeed wheel compiled w. ...... torch 2.1, cuda 12.2
h800-l09: shared memory (/dev/shm) size .... 755.78 GB
h800-l09: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
h800-l09: --------------------------------------------------
h800-l09: DeepSpeed general environment info:
h800-l09: torch install path ............... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/torch']
h800-l09: torch version .................... 2.1.0a0+git7bcf7da
h800-l09: deepspeed install path ........... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/deepspeed']
h800-l09: deepspeed info ................... 0.10.3+04a6fedf, 04a6fedf, HEAD
h800-l09: torch cuda version ............... 12.2
h800-l09: torch hip version ................ None
h800-l09: nvcc version ..................... 12.2
h800-l09: deepspeed wheel compiled w. ...... torch 2.1, cuda 12.2
h800-l09: shared memory (/dev/shm) size .... 755.78 GB
h800-l09: DeepSpeed general environment info:
h800-l09: torch install path ............... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/torch']
h800-l09: torch version .................... 2.1.0a0+git7bcf7da
h800-l09: deepspeed install path ........... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/deepspeed']
h800-l09: deepspeed info ................... 0.10.3+04a6fedf, 04a6fedf, HEAD
h800-l09: torch cuda version ............... 12.2
h800-l09: torch hip version ................ None
h800-l09: nvcc version ..................... 12.2
h800-l09: deepspeed wheel compiled w. ...... torch 2.1, cuda 12.2
h800-l09: shared memory (/dev/shm) size .... 755.78 GB
h800-l07: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: --------------------------------------------------
h800-l09: DeepSpeed general environment info:
h800-l09: torch install path ............... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/torch']
h800-l09: torch version .................... 2.1.0a0+git7bcf7da
h800-l09: deepspeed install path ........... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/deepspeed']
h800-l09: deepspeed info ................... 0.10.3+04a6fedf, 04a6fedf, HEAD
h800-l09: torch cuda version ............... 12.2
h800-l09: torch hip version ................ None
h800-l09: nvcc version ..................... 12.2
h800-l09: deepspeed wheel compiled w. ...... torch 2.1, cuda 12.2
h800-l09: shared memory (/dev/shm) size .... 755.78 GB
h800-l07: DeepSpeed general environment info:
h800-l07: torch install path ............... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/torch']
h800-l07: torch version .................... 2.1.0a0+git7bcf7da
h800-l07: deepspeed install path ........... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/deepspeed']
h800-l07: deepspeed info ................... 0.10.3+04a6fedf, 04a6fedf, HEAD
h800-l07: torch cuda version ............... 12.2
h800-l07: torch hip version ................ None
h800-l07: nvcc version ..................... 12.2
h800-l07: deepspeed wheel compiled w. ...... torch 2.1, cuda 12.2
h800-l07: shared memory (/dev/shm) size .... 755.78 GB
h800-l07: async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
h800-l07: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
h800-l07: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
h800-l07: [93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
h800-l07: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
h800-l09: DeepSpeed general environment info:
h800-l09: torch install path ............... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/torch']
h800-l09: torch version .................... 2.1.0a0+git7bcf7da
h800-l09: deepspeed install path ........... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/deepspeed']
h800-l09: deepspeed info ................... 0.10.3+04a6fedf, 04a6fedf, HEAD
h800-l09: torch cuda version ............... 12.2
h800-l09: torch hip version ................ None
h800-l09: nvcc version ..................... 12.2
h800-l09: deepspeed wheel compiled w. ...... torch 2.1, cuda 12.2
h800-l09: shared memory (/dev/shm) size .... 755.78 GB
h800-l07: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: --------------------------------------------------
h800-l07: DeepSpeed general environment info:
h800-l07: torch install path ............... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/torch']
h800-l07: torch version .................... 2.1.0a0+git7bcf7da
h800-l07: deepspeed install path ........... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/deepspeed']
h800-l07: deepspeed info ................... 0.10.3+04a6fedf, 04a6fedf, HEAD
h800-l07: torch cuda version ............... 12.2
h800-l07: torch hip version ................ None
h800-l07: nvcc version ..................... 12.2
h800-l07: deepspeed wheel compiled w. ...... torch 2.1, cuda 12.2
h800-l07: shared memory (/dev/shm) size .... 755.78 GB
h800-l09: fatal: detected dubious ownership in repository at '/data/nvme3/Megatron-DeepSpeed-patch'
h800-l09: To add an exception for this directory, call:
h800-l09: 
h800-l09: 	git config --global --add safe.directory /data/nvme3/Megatron-DeepSpeed-patch
h800-l09: **** Git info for Megatron: git_hash=unknown git_branch=unknown ****
h800-l09: fatal: detected dubious ownership in repository at '/data/nvme3/Megatron-DeepSpeed-patch'
h800-l09: To add an exception for this directory, call:
h800-l09: 
h800-l09: 	git config --global --add safe.directory /data/nvme3/Megatron-DeepSpeed-patch
h800-l09: **** Git info for Megatron: git_hash=unknown git_branch=unknown ****
h800-l07: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: DeepSpeed general environment info:
h800-l07: torch install path ............... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/torch']
h800-l07: torch version .................... 2.1.0a0+git7bcf7da
h800-l07: deepspeed install path ........... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/deepspeed']
h800-l07: deepspeed info ................... 0.10.3+04a6fedf, 04a6fedf, HEAD
h800-l07: torch cuda version ............... 12.2
h800-l07: torch hip version ................ None
h800-l07: nvcc version ..................... 12.2
h800-l07: deepspeed wheel compiled w. ...... torch 2.1, cuda 12.2
h800-l07: shared memory (/dev/shm) size .... 755.78 GB
h800-l09: fatal: detected dubious ownership in repository at '/data/nvme3/Megatron-DeepSpeed-patch'
h800-l09: To add an exception for this directory, call:
h800-l09: 
h800-l09: 	git config --global --add safe.directory /data/nvme3/Megatron-DeepSpeed-patch
h800-l09: **** Git info for Megatron: git_hash=unknown git_branch=unknown ****
h800-l09: fatal: detected dubious ownership in repository at '/data/nvme3/Megatron-DeepSpeed-patch'
h800-l09: To add an exception for this directory, call:
h800-l09: 
h800-l09: 	git config --global --add safe.directory /data/nvme3/Megatron-DeepSpeed-patch
h800-l09: fatal: detected dubious ownership in repository at '/data/nvme3/Megatron-DeepSpeed-patch'
h800-l09: To add an exception for this directory, call:
h800-l09: 
h800-l09: 	git config --global --add safe.directory /data/nvme3/Megatron-DeepSpeed-patch
h800-l09: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
h800-l09: **** Git info for Megatron: git_hash=unknown git_branch=unknown ****
h800-l09: **** Git info for Megatron: git_hash=unknown git_branch=unknown ****
h800-l09: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
h800-l07: fatal: detected dubious ownership in repository at '/data/nvme3/Megatron-DeepSpeed-patch'
h800-l07: To add an exception for this directory, call:
h800-l07: 
h800-l07: 	git config --global --add safe.directory /data/nvme3/Megatron-DeepSpeed-patch
h800-l07: **** Git info for Megatron: git_hash=unknown git_branch=unknown ****
h800-l09: fatal: detected dubious ownership in repository at '/data/nvme3/Megatron-DeepSpeed-patch'
h800-l09: To add an exception for this directory, call:
h800-l09: 
h800-l09: 	git config --global --add safe.directory /data/nvme3/Megatron-DeepSpeed-patch
h800-l09: **** Git info for Megatron: git_hash=unknown git_branch=unknown ****
h800-l07: fatal: detected dubious ownership in repository at '/data/nvme3/Megatron-DeepSpeed-patch'
h800-l07: To add an exception for this directory, call:
h800-l07: 
h800-l07: 	git config --global --add safe.directory /data/nvme3/Megatron-DeepSpeed-patch
h800-l07: **** Git info for Megatron: git_hash=unknown git_branch=unknown ****
h800-l07: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
h800-l07: --------------------------------------------------
h800-l09: fatal: detected dubious ownership in repository at '/data/nvme3/Megatron-DeepSpeed-patch'
h800-l09: To add an exception for this directory, call:
h800-l09: 
h800-l09: 	git config --global --add safe.directory /data/nvme3/Megatron-DeepSpeed-patch
h800-l09: **** Git info for Megatron: git_hash=unknown git_branch=unknown ****
h800-l09: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
h800-l09: [2024-05-08 16:03:55,143] [INFO] [comm.py:637:init_distributed] cdb=None
h800-l09: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
h800-l09: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
h800-l07: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
h800-l07: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
h800-l09: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
h800-l07: fatal: detected dubious ownership in repository at '/data/nvme3/Megatron-DeepSpeed-patch'
h800-l07: To add an exception for this directory, call:
h800-l07: 
h800-l07: 	git config --global --add safe.directory /data/nvme3/Megatron-DeepSpeed-patch
h800-l07: **** Git info for Megatron: git_hash=unknown git_branch=unknown ****
h800-l07: DeepSpeed general environment info:
h800-l07: torch install path ............... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/torch']
h800-l07: torch version .................... 2.1.0a0+git7bcf7da
h800-l07: deepspeed install path ........... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/deepspeed']
h800-l07: deepspeed info ................... 0.10.3+04a6fedf, 04a6fedf, HEAD
h800-l07: torch cuda version ............... 12.2
h800-l07: torch hip version ................ None
h800-l07: nvcc version ..................... 12.2
h800-l07: deepspeed wheel compiled w. ...... torch 2.1, cuda 12.2
h800-l07: shared memory (/dev/shm) size .... 755.78 GB
h800-l09: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
h800-l07: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
h800-l07: fatal: detected dubious ownership in repository at '/data/nvme3/Megatron-DeepSpeed-patch'
h800-l07: To add an exception for this directory, call:
h800-l07: 
h800-l07: 	git config --global --add safe.directory /data/nvme3/Megatron-DeepSpeed-patch
h800-l07: **** Git info for Megatron: git_hash=unknown git_branch=unknown ****
h800-l07: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
h800-l07: [2024-05-08 16:03:55,506] [INFO] [comm.py:637:init_distributed] cdb=None
h800-l07: [2024-05-08 16:03:55,723] [INFO] [comm.py:637:init_distributed] cdb=None
h800-l07: [2024-05-08 16:03:55,826] [INFO] [comm.py:637:init_distributed] cdb=None
h800-l07: [2024-05-08 16:03:55,839] [INFO] [comm.py:637:init_distributed] cdb=None
h800-l07: [2024-05-08 16:03:55,855] [INFO] [comm.py:637:init_distributed] cdb=None
h800-l07: [2024-05-08 16:03:55,863] [INFO] [comm.py:637:init_distributed] cdb=None
h800-l07: [2024-05-08 16:03:55,865] [INFO] [comm.py:637:init_distributed] cdb=None
h800-l09: [2024-05-08 16:03:55,996] [INFO] [comm.py:637:init_distributed] cdb=None
h800-l09: [2024-05-08 16:03:56,014] [INFO] [comm.py:637:init_distributed] cdb=None
h800-l09: [2024-05-08 16:03:56,015] [INFO] [comm.py:637:init_distributed] cdb=None
h800-l09: [2024-05-08 16:03:56,016] [INFO] [comm.py:637:init_distributed] cdb=None
h800-l09: [2024-05-08 16:03:56,018] [INFO] [comm.py:637:init_distributed] cdb=None
h800-l09: [2024-05-08 16:03:56,030] [INFO] [comm.py:637:init_distributed] cdb=None
h800-l03: /data/nvme3/Megatron-DeepSpeed/megatron/model/module.py:13: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /data/nvme5/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
h800-l03:   _FLOAT_TYPES = [get_accelerator().FloatTensor(0).dtype]
h800-l03: --------------------------------------------------
h800-l03: DeepSpeed C++/CUDA extension op report
h800-l03: --------------------------------------------------
h800-l03: NOTE: Ops not installed will be just-in-time (JIT) compiled at
h800-l03:       runtime if needed. Op compatibility means that your system
h800-l03:       meet the required dependencies to JIT install the op.
h800-l03: --------------------------------------------------
h800-l03: JIT compiled ops requires ninja
h800-l03: ninja .................. [92m[OKAY][0m
h800-l03: --------------------------------------------------
h800-l03: op name ................ installed .. compatible
h800-l03: --------------------------------------------------
h800-l03: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
h800-l03: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
h800-l03: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
h800-l03: async_io ............... [93m[NO][0m ....... [93m[NO][0m
h800-l03: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
h800-l03: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
h800-l03: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
h800-l03: [93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
h800-l03: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
h800-l03: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: --------------------------------------------------
h800-l03: DeepSpeed general environment info:
h800-l03: torch install path ............... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/torch']
h800-l03: torch version .................... 2.1.0a0+git7bcf7da
h800-l03: deepspeed install path ........... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/deepspeed']
h800-l03: deepspeed info ................... 0.10.3+04a6fedf, 04a6fedf, HEAD
h800-l03: torch cuda version ............... 12.2
h800-l03: torch hip version ................ None
h800-l03: nvcc version ..................... 12.2
h800-l03: deepspeed wheel compiled w. ...... torch 2.1, cuda 12.2
h800-l03: shared memory (/dev/shm) size .... 755.78 GB
h800-l03: fatal: detected dubious ownership in repository at '/data/nvme3/Megatron-DeepSpeed-patch'
h800-l03: To add an exception for this directory, call:
h800-l03: 
h800-l03: 	git config --global --add safe.directory /data/nvme3/Megatron-DeepSpeed-patch
h800-l03: **** Git info for Megatron: git_hash=unknown git_branch=unknown ****
h800-l03: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
h800-l03: /data/nvme3/Megatron-DeepSpeed/megatron/model/module.py:13: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /data/nvme5/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
h800-l03:   _FLOAT_TYPES = [get_accelerator().FloatTensor(0).dtype]
h800-l03: --------------------------------------------------
h800-l03: DeepSpeed C++/CUDA extension op report
h800-l03: --------------------------------------------------
h800-l03: NOTE: Ops not installed will be just-in-time (JIT) compiled at
h800-l03:       runtime if needed. Op compatibility means that your system
h800-l03:       meet the required dependencies to JIT install the op.
h800-l03: --------------------------------------------------
h800-l03: JIT compiled ops requires ninja
h800-l03: ninja .................. [92m[OKAY][0m
h800-l03: --------------------------------------------------
h800-l03: op name ................ installed .. compatible
h800-l03: --------------------------------------------------
h800-l03: [2024-05-08 16:03:57,044] [INFO] [comm.py:637:init_distributed] cdb=None
h800-l03: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
h800-l03: /data/nvme3/Megatron-DeepSpeed/megatron/model/module.py:13: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /data/nvme5/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
h800-l03:   _FLOAT_TYPES = [get_accelerator().FloatTensor(0).dtype]
h800-l03: /data/nvme3/Megatron-DeepSpeed/megatron/model/module.py:13: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /data/nvme5/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
h800-l03:   _FLOAT_TYPES = [get_accelerator().FloatTensor(0).dtype]
h800-l03: /data/nvme3/Megatron-DeepSpeed/megatron/model/module.py:13: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /data/nvme5/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
h800-l03:   _FLOAT_TYPES = [get_accelerator().FloatTensor(0).dtype]
h800-l03: /data/nvme3/Megatron-DeepSpeed/megatron/model/module.py:13: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /data/nvme5/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
h800-l03:   _FLOAT_TYPES = [get_accelerator().FloatTensor(0).dtype]
h800-l03: --------------------------------------------------
h800-l03: DeepSpeed C++/CUDA extension op report
h800-l03: --------------------------------------------------
h800-l03: NOTE: Ops not installed will be just-in-time (JIT) compiled at
h800-l03:       runtime if needed. Op compatibility means that your system
h800-l03:       meet the required dependencies to JIT install the op.
h800-l03: --------------------------------------------------
h800-l03: JIT compiled ops requires ninja
h800-l03: ninja .................. [92m[OKAY][0m
h800-l03: --------------------------------------------------
h800-l03: op name ................ installed .. compatible
h800-l03: --------------------------------------------------
h800-l03: --------------------------------------------------
h800-l03: DeepSpeed C++/CUDA extension op report
h800-l03: --------------------------------------------------
h800-l03: NOTE: Ops not installed will be just-in-time (JIT) compiled at
h800-l03:       runtime if needed. Op compatibility means that your system
h800-l03:       meet the required dependencies to JIT install the op.
h800-l03: --------------------------------------------------
h800-l03: JIT compiled ops requires ninja
h800-l03: ninja .................. [92m[OKAY][0m
h800-l03: --------------------------------------------------
h800-l03: op name ................ installed .. compatible
h800-l03: --------------------------------------------------
h800-l03: --------------------------------------------------
h800-l03: DeepSpeed C++/CUDA extension op report
h800-l03: --------------------------------------------------
h800-l03: NOTE: Ops not installed will be just-in-time (JIT) compiled at
h800-l03:       runtime if needed. Op compatibility means that your system
h800-l03:       meet the required dependencies to JIT install the op.
h800-l03: --------------------------------------------------
h800-l03: JIT compiled ops requires ninja
h800-l03: --------------------------------------------------
h800-l03: DeepSpeed C++/CUDA extension op report
h800-l03: --------------------------------------------------
h800-l03: NOTE: Ops not installed will be just-in-time (JIT) compiled at
h800-l03:       runtime if needed. Op compatibility means that your system
h800-l03:       meet the required dependencies to JIT install the op.
h800-l03: --------------------------------------------------
h800-l03: JIT compiled ops requires ninja
h800-l03: ninja .................. [92m[OKAY][0m
h800-l03: --------------------------------------------------
h800-l03: op name ................ installed .. compatible
h800-l03: --------------------------------------------------
h800-l03: ninja .................. [92m[OKAY][0m
h800-l03: --------------------------------------------------
h800-l03: op name ................ installed .. compatible
h800-l03: --------------------------------------------------
h800-l03: /data/nvme3/Megatron-DeepSpeed/megatron/model/module.py:13: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /data/nvme5/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
h800-l03:   _FLOAT_TYPES = [get_accelerator().FloatTensor(0).dtype]
h800-l03: /data/nvme3/Megatron-DeepSpeed/megatron/model/module.py:13: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /data/nvme5/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
h800-l03:   _FLOAT_TYPES = [get_accelerator().FloatTensor(0).dtype]
h800-l03: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
h800-l03: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
h800-l03: async_io ............... [93m[NO][0m ....... [93m[NO][0m
h800-l03: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
h800-l03: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
h800-l03: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
h800-l03: [93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
h800-l03: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
h800-l03: --------------------------------------------------
h800-l03: DeepSpeed C++/CUDA extension op report
h800-l03: --------------------------------------------------
h800-l03: NOTE: Ops not installed will be just-in-time (JIT) compiled at
h800-l03:       runtime if needed. Op compatibility means that your system
h800-l03:       meet the required dependencies to JIT install the op.
h800-l03: --------------------------------------------------
h800-l03: JIT compiled ops requires ninja
h800-l03: ninja .................. [92m[OKAY][0m
h800-l03: --------------------------------------------------
h800-l03: op name ................ installed .. compatible
h800-l03: --------------------------------------------------
h800-l03: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
h800-l03: --------------------------------------------------
h800-l03: DeepSpeed C++/CUDA extension op report
h800-l03: --------------------------------------------------
h800-l03: NOTE: Ops not installed will be just-in-time (JIT) compiled at
h800-l03:       runtime if needed. Op compatibility means that your system
h800-l03:       meet the required dependencies to JIT install the op.
h800-l03: --------------------------------------------------
h800-l03: JIT compiled ops requires ninja
h800-l03: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: ninja .................. [92m[OKAY][0m
h800-l03: --------------------------------------------------
h800-l03: op name ................ installed .. compatible
h800-l03: --------------------------------------------------
h800-l03: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
h800-l03: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
h800-l03: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: --------------------------------------------------
h800-l03: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
h800-l03: DeepSpeed general environment info:
h800-l03: torch install path ............... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/torch']
h800-l03: torch version .................... 2.1.0a0+git7bcf7da
h800-l03: deepspeed install path ........... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/deepspeed']
h800-l03: deepspeed info ................... 0.10.3+04a6fedf, 04a6fedf, HEAD
h800-l03: torch cuda version ............... 12.2
h800-l03: torch hip version ................ None
h800-l03: nvcc version ..................... 12.2
h800-l03: deepspeed wheel compiled w. ...... torch 2.1, cuda 12.2
h800-l03: shared memory (/dev/shm) size .... 755.78 GB
h800-l03: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
h800-l03: fatal: detected dubious ownership in repository at '/data/nvme3/Megatron-DeepSpeed-patch'
h800-l03: To add an exception for this directory, call:
h800-l03: 
h800-l03: 	git config --global --add safe.directory /data/nvme3/Megatron-DeepSpeed-patch
h800-l03: **** Git info for Megatron: git_hash=unknown git_branch=unknown ****
h800-l03: [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
h800-l03: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
h800-l03: [2024-05-08 16:03:57,293] [INFO] [comm.py:637:init_distributed] cdb=None
h800-l03: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
h800-l03: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
h800-l03: async_io ............... [93m[NO][0m ....... [93m[NO][0m
h800-l03: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
h800-l03: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
h800-l03: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
h800-l03: [93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
h800-l03: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
h800-l03: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
h800-l03: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
h800-l03: async_io ............... [93m[NO][0m ....... [93m[NO][0m
h800-l03: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
h800-l03: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
h800-l03: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
h800-l03: [93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
h800-l03: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
h800-l03: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
h800-l03: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
h800-l03: async_io ............... [93m[NO][0m ....... [93m[NO][0m
h800-l03: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
h800-l03: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
h800-l03: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
h800-l03: [93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
h800-l03: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
h800-l03: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: --------------------------------------------------
h800-l03: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
h800-l03: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
h800-l03: async_io ............... [93m[NO][0m ....... [93m[NO][0m
h800-l03: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
h800-l03: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
h800-l03: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
h800-l03: [93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
h800-l03: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
h800-l03: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: --------------------------------------------------
h800-l03: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: DeepSpeed general environment info:
h800-l03: torch install path ............... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/torch']
h800-l03: torch version .................... 2.1.0a0+git7bcf7da
h800-l03: deepspeed install path ........... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/deepspeed']
h800-l03: deepspeed info ................... 0.10.3+04a6fedf, 04a6fedf, HEAD
h800-l03: torch cuda version ............... 12.2
h800-l03: torch hip version ................ None
h800-l03: nvcc version ..................... 12.2
h800-l03: deepspeed wheel compiled w. ...... torch 2.1, cuda 12.2
h800-l03: shared memory (/dev/shm) size .... 755.78 GB
h800-l03: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: DeepSpeed general environment info:
h800-l03: torch install path ............... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/torch']
h800-l03: torch version .................... 2.1.0a0+git7bcf7da
h800-l03: deepspeed install path ........... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/deepspeed']
h800-l03: deepspeed info ................... 0.10.3+04a6fedf, 04a6fedf, HEAD
h800-l03: torch cuda version ............... 12.2
h800-l03: torch hip version ................ None
h800-l03: nvcc version ..................... 12.2
h800-l03: deepspeed wheel compiled w. ...... torch 2.1, cuda 12.2
h800-l03: shared memory (/dev/shm) size .... 755.78 GB
h800-l03: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: --------------------------------------------------
h800-l03: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
h800-l03: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
h800-l03: async_io ............... [93m[NO][0m ....... [93m[NO][0m
h800-l03: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
h800-l03: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
h800-l03: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
h800-l03: [93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
h800-l03: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
h800-l03: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: --------------------------------------------------
h800-l03: fatal: detected dubious ownership in repository at '/data/nvme3/Megatron-DeepSpeed-patch'
h800-l03: To add an exception for this directory, call:
h800-l03: 
h800-l03: 	git config --global --add safe.directory /data/nvme3/Megatron-DeepSpeed-patch
h800-l03: **** Git info for Megatron: git_hash=unknown git_branch=unknown ****
h800-l03: DeepSpeed general environment info:
h800-l03: torch install path ............... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/torch']
h800-l03: torch version .................... 2.1.0a0+git7bcf7da
h800-l03: deepspeed install path ........... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/deepspeed']
h800-l03: deepspeed info ................... 0.10.3+04a6fedf, 04a6fedf, HEAD
h800-l03: torch cuda version ............... 12.2
h800-l03: torch hip version ................ None
h800-l03: nvcc version ..................... 12.2
h800-l03: deepspeed wheel compiled w. ...... torch 2.1, cuda 12.2
h800-l03: shared memory (/dev/shm) size .... 755.78 GB
h800-l03: DeepSpeed general environment info:
h800-l03: torch install path ............... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/torch']
h800-l03: torch version .................... 2.1.0a0+git7bcf7da
h800-l03: deepspeed install path ........... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/deepspeed']
h800-l03: deepspeed info ................... 0.10.3+04a6fedf, 04a6fedf, HEAD
h800-l03: torch cuda version ............... 12.2
h800-l03: torch hip version ................ None
h800-l03: nvcc version ..................... 12.2
h800-l03: deepspeed wheel compiled w. ...... torch 2.1, cuda 12.2
h800-l03: shared memory (/dev/shm) size .... 755.78 GB
h800-l03: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: fatal: detected dubious ownership in repository at '/data/nvme3/Megatron-DeepSpeed-patch'
h800-l03: To add an exception for this directory, call:
h800-l03: 
h800-l03: 	git config --global --add safe.directory /data/nvme3/Megatron-DeepSpeed-patch
h800-l03: **** Git info for Megatron: git_hash=unknown git_branch=unknown ****
h800-l03: [93m [WARNING] [0m async_io: please install the libaio-dev package with apt
h800-l03: [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
h800-l03: async_io ............... [93m[NO][0m ....... [93m[NO][0m
h800-l03: fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: cpu_lion ............... [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
h800-l03: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
h800-l03: fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: fused_lion ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
h800-l03: [93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
h800-l03: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
h800-l03: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: --------------------------------------------------
h800-l03: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
h800-l03: fatal: detected dubious ownership in repository at '/data/nvme3/Megatron-DeepSpeed-patch'
h800-l03: To add an exception for this directory, call:
h800-l03: 
h800-l03: 	git config --global --add safe.directory /data/nvme3/Megatron-DeepSpeed-patch
h800-l03: **** Git info for Megatron: git_hash=unknown git_branch=unknown ****
h800-l03: fatal: detected dubious ownership in repository at '/data/nvme3/Megatron-DeepSpeed-patch'
h800-l03: To add an exception for this directory, call:
h800-l03: 
h800-l03: 	git config --global --add safe.directory /data/nvme3/Megatron-DeepSpeed-patch
h800-l03: **** Git info for Megatron: git_hash=unknown git_branch=unknown ****
h800-l03: spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
h800-l03: DeepSpeed general environment info:
h800-l03: torch install path ............... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/torch']
h800-l03: torch version .................... 2.1.0a0+git7bcf7da
h800-l03: deepspeed install path ........... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/deepspeed']
h800-l03: deepspeed info ................... 0.10.3+04a6fedf, 04a6fedf, HEAD
h800-l03: torch cuda version ............... 12.2
h800-l03: torch hip version ................ None
h800-l03: nvcc version ..................... 12.2
h800-l03: deepspeed wheel compiled w. ...... torch 2.1, cuda 12.2
h800-l03: shared memory (/dev/shm) size .... 755.78 GB
h800-l03: transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
h800-l03: --------------------------------------------------
h800-l03: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
h800-l03: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
h800-l03: DeepSpeed general environment info:
h800-l03: torch install path ............... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/torch']
h800-l03: torch version .................... 2.1.0a0+git7bcf7da
h800-l03: deepspeed install path ........... ['/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/deepspeed']
h800-l03: deepspeed info ................... 0.10.3+04a6fedf, 04a6fedf, HEAD
h800-l03: torch cuda version ............... 12.2
h800-l03: torch hip version ................ None
h800-l03: nvcc version ..................... 12.2
h800-l03: deepspeed wheel compiled w. ...... torch 2.1, cuda 12.2
h800-l03: shared memory (/dev/shm) size .... 755.78 GB
h800-l03: fatal: detected dubious ownership in repository at '/data/nvme3/Megatron-DeepSpeed-patch'
h800-l03: To add an exception for this directory, call:
h800-l03: 
h800-l03: 	git config --global --add safe.directory /data/nvme3/Megatron-DeepSpeed-patch
h800-l03: **** Git info for Megatron: git_hash=unknown git_branch=unknown ****
h800-l03: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
h800-l03: fatal: detected dubious ownership in repository at '/data/nvme3/Megatron-DeepSpeed-patch'
h800-l03: To add an exception for this directory, call:
h800-l03: 
h800-l03: 	git config --global --add safe.directory /data/nvme3/Megatron-DeepSpeed-patch
h800-l03: **** Git info for Megatron: git_hash=unknown git_branch=unknown ****
h800-l03: Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
h800-l03: [2024-05-08 16:03:58,212] [INFO] [comm.py:637:init_distributed] cdb=None
h800-l03: [2024-05-08 16:03:58,241] [INFO] [comm.py:637:init_distributed] cdb=None
h800-l03: [2024-05-08 16:03:58,273] [INFO] [comm.py:637:init_distributed] cdb=None
h800-l03: [2024-05-08 16:03:58,273] [INFO] [comm.py:637:init_distributed] cdb=None
h800-l03: [2024-05-08 16:03:58,277] [INFO] [comm.py:637:init_distributed] cdb=None
h800-l03: [2024-05-08 16:03:58,824] [INFO] [comm.py:637:init_distributed] cdb=None
h800-l05: > initialized tensor model parallel with size 1
h800-l05: > initialized pipeline model parallel with size 8
h800-l05: > setting random seeds to 1234 ...
h800-l05: [2024-05-08 16:03:58,789] [INFO] [checkpointing.py:227:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
h800-l05: > compiling dataset index builder ...
h800-l05: make: Entering directory '/data/nvme3/Megatron-DeepSpeed/megatron/data'
h800-l05: make: Nothing to be done for 'default'.
h800-l05: make: Leaving directory '/data/nvme3/Megatron-DeepSpeed/megatron/data'
h800-l05: >>> done with dataset index builder. Compilation time: 0.056 seconds
h800-l05: WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
h800-l05: > compiling and loading fused kernels ...
h800-l05: Detected CUDA files, patching ldflags
h800-l05: Emitting ninja build file /data/nvme3/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
h800-l05: Building extension module scaled_upper_triang_masked_softmax_cuda...
h800-l05: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
h800-l05: [1/3] c++ -MMD -MF scaled_upper_triang_masked_softmax.o.d -DTORCH_EXTENSION_NAME=scaled_upper_triang_masked_softmax_cuda -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1013\" -isystem /home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/torch/include -isystem /home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -isystem /home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/torch/include/TH -isystem /home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/cestc/miniconda3/envs/ds/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++17 -O3 -c /data/nvme3/Megatron-DeepSpeed/megatron/fused_kernels/scaled_upper_triang_masked_softmax.cpp -o scaled_upper_triang_masked_softmax.o 
h800-l05: [2/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=scaled_upper_triang_masked_softmax_cuda -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1013\" -isystem /home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/torch/include -isystem /home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -isystem /home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/torch/include/TH -isystem /home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/cestc/miniconda3/envs/ds/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -O3 -gencode arch=compute_70,code=sm_70 --use_fast_math -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -std=c++17 -c /data/nvme3/Megatron-DeepSpeed/megatron/fused_kernels/scaled_upper_triang_masked_softmax_cuda.cu -o scaled_upper_triang_masked_softmax_cuda.cuda.o 
h800-l05: [3/3] c++ scaled_upper_triang_masked_softmax.o scaled_upper_triang_masked_softmax_cuda.cuda.o -shared -L/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o scaled_upper_triang_masked_softmax_cuda.so
h800-l05: Loading extension module scaled_upper_triang_masked_softmax_cuda...
h800-l05: Detected CUDA files, patching ldflags
h800-l05: Emitting ninja build file /data/nvme3/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
h800-l05: Building extension module scaled_masked_softmax_cuda...
h800-l05: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
h800-l05: [1/3] c++ -MMD -MF scaled_masked_softmax.o.d -DTORCH_EXTENSION_NAME=scaled_masked_softmax_cuda -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1013\" -isystem /home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/torch/include -isystem /home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -isystem /home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/torch/include/TH -isystem /home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/cestc/miniconda3/envs/ds/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++17 -O3 -c /data/nvme3/Megatron-DeepSpeed/megatron/fused_kernels/scaled_masked_softmax.cpp -o scaled_masked_softmax.o 
h800-l05: [2/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=scaled_masked_softmax_cuda -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1013\" -isystem /home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/torch/include -isystem /home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -isystem /home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/torch/include/TH -isystem /home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/cestc/miniconda3/envs/ds/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -O3 -gencode arch=compute_70,code=sm_70 --use_fast_math -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -std=c++17 -c /data/nvme3/Megatron-DeepSpeed/megatron/fused_kernels/scaled_masked_softmax_cuda.cu -o scaled_masked_softmax_cuda.cuda.o 
h800-l05: [3/3] c++ scaled_masked_softmax.o scaled_masked_softmax_cuda.cuda.o -shared -L/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o scaled_masked_softmax_cuda.so
h800-l05: Loading extension module scaled_masked_softmax_cuda...
h800-l05: Detected CUDA files, patching ldflags
h800-l05: Emitting ninja build file /data/nvme3/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
h800-l05: Building extension module scaled_softmax_cuda...
h800-l05: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
h800-l05: [1/3] c++ -MMD -MF scaled_softmax.o.d -DTORCH_EXTENSION_NAME=scaled_softmax_cuda -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1013\" -isystem /home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/torch/include -isystem /home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -isystem /home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/torch/include/TH -isystem /home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/cestc/miniconda3/envs/ds/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++17 -O3 -c /data/nvme3/Megatron-DeepSpeed/megatron/fused_kernels/scaled_softmax.cpp -o scaled_softmax.o 
h800-l05: [2/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=scaled_softmax_cuda -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1013\" -isystem /home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/torch/include -isystem /home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -isystem /home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/torch/include/TH -isystem /home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/cestc/miniconda3/envs/ds/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -O3 -gencode arch=compute_70,code=sm_70 --use_fast_math -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -std=c++17 -c /data/nvme3/Megatron-DeepSpeed/megatron/fused_kernels/scaled_softmax_cuda.cu -o scaled_softmax_cuda.cuda.o 
h800-l05: [3/3] c++ scaled_softmax.o scaled_softmax_cuda.cuda.o -shared -L/home/cestc/miniconda3/envs/ds/lib/python3.8/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o scaled_softmax_cuda.so
h800-l05: Loading extension module scaled_softmax_cuda...
h800-l05: >>> done with compiling and loading fused kernels. Compilation time: 376.984 seconds
h800-l05: /data/nvme3/Megatron-DeepSpeed/megatron/initialize.py:351: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /data/nvme5/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
h800-l05:   output = bias_gelu(bias, input)
h800-l05: /data/nvme3/Megatron-DeepSpeed/megatron/initialize.py:351: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /data/nvme5/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
h800-l05:   output = bias_gelu(bias, input)
h800-l05: /data/nvme3/Megatron-DeepSpeed/megatron/initialize.py:351: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /data/nvme5/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
h800-l05:   output = bias_gelu(bias, input)
h800-l05: /data/nvme3/Megatron-DeepSpeed/megatron/initialize.py:351: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /data/nvme5/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
h800-l05:   output = bias_gelu(bias, input)
h800-l03: /data/nvme3/Megatron-DeepSpeed/megatron/initialize.py:351: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /data/nvme5/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
h800-l03:   output = bias_gelu(bias, input)
h800-l03: /data/nvme3/Megatron-DeepSpeed/megatron/initialize.py:351: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /data/nvme5/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
h800-l03:   output = bias_gelu(bias, input)
h800-l03: /data/nvme3/Megatron-DeepSpeed/megatron/initialize.py:351: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /data/nvme5/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
h800-l03:   output = bias_gelu(bias, input)
h800-l03: /data/nvme3/Megatron-DeepSpeed/megatron/initialize.py:351: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /data/nvme5/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
h800-l03:   output = bias_gelu(bias, input)
h800-l05: /data/nvme3/Megatron-DeepSpeed/megatron/initialize.py:351: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /data/nvme5/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
h800-l05:   output = bias_gelu(bias, input)
h800-l03: /data/nvme3/Megatron-DeepSpeed/megatron/initialize.py:351: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /data/nvme5/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
h800-l03:   output = bias_gelu(bias, input)
h800-l07: /data/nvme3/Megatron-DeepSpeed/megatron/initialize.py:351: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /data/nvme5/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
h800-l07:   output = bias_gelu(bias, input)
h800-l07: /data/nvme3/Megatron-DeepSpeed/megatron/initialize.py:351: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /data/nvme5/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
h800-l07:   output = bias_gelu(bias, input)
h800-l07: /data/nvme3/Megatron-DeepSpeed/megatron/initialize.py:351: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /data/nvme5/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
h800-l07:   output = bias_gelu(bias, input)
h800-l05: /data/nvme3/Megatron-DeepSpeed/megatron/initialize.py:351: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /data/nvme5/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
h800-l05:   output = bias_gelu(bias, input)
h800-l07: /data/nvme3/Megatron-DeepSpeed/megatron/initialize.py:351: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /data/nvme5/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
h800-l07:   output = bias_gelu(bias, input)
h800-l07: /data/nvme3/Megatron-DeepSpeed/megatron/initialize.py:351: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /data/nvme5/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
h800-l07:   output = bias_gelu(bias, input)
h800-l05: /data/nvme3/Megatron-DeepSpeed/megatron/initialize.py:351: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /data/nvme5/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
h800-l05:   output = bias_gelu(bias, input)
h800-l05: /data/nvme3/Megatron-DeepSpeed/megatron/initialize.py:351: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /data/nvme5/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
h800-l05:   output = bias_gelu(bias, input)
h800-l07: /data/nvme3/Megatron-DeepSpeed/megatron/initialize.py:351: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /data/nvme5/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
h800-l07:   output = bias_gelu(bias, input)
h800-l09: /data/nvme3/Megatron-DeepSpeed/megatron/initialize.py:351: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /data/nvme5/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
h800-l09:   output = bias_gelu(bias, input)
h800-l09: /data/nvme3/Megatron-DeepSpeed/megatron/initialize.py:351: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /data/nvme5/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
h800-l09:   output = bias_gelu(bias, input)
h800-l07: /data/nvme3/Megatron-DeepSpeed/megatron/initialize.py:351: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /data/nvme5/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
h800-l07:   output = bias_gelu(bias, input)
h800-l07: /data/nvme3/Megatron-DeepSpeed/megatron/initialize.py:351: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /data/nvme5/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
h800-l07:   output = bias_gelu(bias, input)
h800-l09: /data/nvme3/Megatron-DeepSpeed/megatron/initialize.py:351: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /data/nvme5/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
h800-l09:   output = bias_gelu(bias, input)
h800-l03: /data/nvme3/Megatron-DeepSpeed/megatron/initialize.py:351: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /data/nvme5/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
h800-l03:   output = bias_gelu(bias, input)
h800-l03: /data/nvme3/Megatron-DeepSpeed/megatron/initialize.py:351: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /data/nvme5/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
h800-l03:   output = bias_gelu(bias, input)
h800-l03: /data/nvme3/Megatron-DeepSpeed/megatron/initialize.py:351: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /data/nvme5/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
h800-l03:   output = bias_gelu(bias, input)
h800-l09: /data/nvme3/Megatron-DeepSpeed/megatron/initialize.py:351: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /data/nvme5/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
h800-l09:   output = bias_gelu(bias, input)
h800-l09: /data/nvme3/Megatron-DeepSpeed/megatron/initialize.py:351: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /data/nvme5/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
h800-l09:   output = bias_gelu(bias, input)
h800-l09: /data/nvme3/Megatron-DeepSpeed/megatron/initialize.py:351: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /data/nvme5/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
h800-l09:   output = bias_gelu(bias, input)
h800-l09: /data/nvme3/Megatron-DeepSpeed/megatron/initialize.py:351: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /data/nvme5/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
h800-l09:   output = bias_gelu(bias, input)
h800-l09: /data/nvme3/Megatron-DeepSpeed/megatron/initialize.py:351: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /data/nvme5/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
h800-l09:   output = bias_gelu(bias, input)
h800-l05: time to initialize megatron (seconds): 384.737
h800-l05: [after megatron is initialized] datetime: 2024-05-08 16:10:16 
h800-l05: building GPT model ...
h800-l05: [2024-05-08 16:10:16,643] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l05: [2024-05-08 16:10:16,643] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l03: [2024-05-08 16:10:16,690] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l09: [2024-05-08 16:10:16,664] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l09: [2024-05-08 16:10:16,664] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l09: [2024-05-08 16:10:16,664] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l07: [2024-05-08 16:10:16,689] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l09: [2024-05-08 16:10:16,664] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l05: [2024-05-08 16:10:16,644] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l07: [2024-05-08 16:10:16,689] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l03: [2024-05-08 16:10:16,690] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l07: [2024-05-08 16:10:16,689] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l07: [2024-05-08 16:10:16,689] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l07: [2024-05-08 16:10:16,689] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l03: [2024-05-08 16:10:16,690] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l03: [2024-05-08 16:10:16,690] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l03: [2024-05-08 16:10:16,691] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l05: [2024-05-08 16:10:16,644] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l05: [2024-05-08 16:10:16,644] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l05: [2024-05-08 16:10:16,644] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l03: [2024-05-08 16:10:16,691] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l05: [2024-05-08 16:10:16,644] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l03: [2024-05-08 16:10:16,691] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l07: [2024-05-08 16:10:16,690] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l09: [2024-05-08 16:10:16,664] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l03: [2024-05-08 16:10:16,691] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l09: [2024-05-08 16:10:16,664] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l09: [2024-05-08 16:10:16,665] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l07: [2024-05-08 16:10:16,690] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l07: [2024-05-08 16:10:16,690] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l09: [2024-05-08 16:10:16,666] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l05: [2024-05-08 16:10:16,725] [INFO] [utils.py:803:see_memory_usage] Before Building Model
h800-l05: [2024-05-08 16:10:16,725] [INFO] [utils.py:804:see_memory_usage] MA 0.0 GB         Max_MA 8.5 GB         CA 0.0 GB         Max_CA 9 GB 
h800-l05: [2024-05-08 16:10:16,726] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 28.42 GB, percent = 1.9%
h800-l05: [2024-05-08 16:10:16,726] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l05: SEED_LAYERS=False BASE_SEED=1234 SEED_FN=None
h800-l05: Using topology: {ProcessCoord(pipe=0, data=0, model=0): 0, ProcessCoord(pipe=0, data=1, model=0): 1, ProcessCoord(pipe=0, data=2, model=0): 2, ProcessCoord(pipe=0, data=3, model=0): 3, ProcessCoord(pipe=1, data=0, model=0): 4, ProcessCoord(pipe=1, data=1, model=0): 5, ProcessCoord(pipe=1, data=2, model=0): 6, ProcessCoord(pipe=1, data=3, model=0): 7, ProcessCoord(pipe=2, data=0, model=0): 8, ProcessCoord(pipe=2, data=1, model=0): 9, ProcessCoord(pipe=2, data=2, model=0): 10, ProcessCoord(pipe=2, data=3, model=0): 11, ProcessCoord(pipe=3, data=0, model=0): 12, ProcessCoord(pipe=3, data=1, model=0): 13, ProcessCoord(pipe=3, data=2, model=0): 14, ProcessCoord(pipe=3, data=3, model=0): 15, ProcessCoord(pipe=4, data=0, model=0): 16, ProcessCoord(pipe=4, data=1, model=0): 17, ProcessCoord(pipe=4, data=2, model=0): 18, ProcessCoord(pipe=4, data=3, model=0): 19, ProcessCoord(pipe=5, data=0, model=0): 20, ProcessCoord(pipe=5, data=1, model=0): 21, ProcessCoord(pipe=5, data=2, model=0): 22, ProcessCoord(pipe=5, data=3, model=0): 23, ProcessCoord(pipe=6, data=0, model=0): 24, ProcessCoord(pipe=6, data=1, model=0): 25, ProcessCoord(pipe=6, data=2, model=0): 26, ProcessCoord(pipe=6, data=3, model=0): 27, ProcessCoord(pipe=7, data=0, model=0): 28, ProcessCoord(pipe=7, data=1, model=0): 29, ProcessCoord(pipe=7, data=2, model=0): 30, ProcessCoord(pipe=7, data=3, model=0): 31}
h800-l05: [2024-05-08 16:10:16,729] [INFO] [module.py:375:_partition_layers] Partitioning pipeline stages with method type:transformer
h800-l05: stage=0 layers=12
h800-l05:      0: _to_float16
h800-l05:      1: EmbeddingPipe
h800-l05:      2: ParallelTransformerLayerPipe
h800-l05:      3: ParallelTransformerLayerPipe
h800-l05:      4: ParallelTransformerLayerPipe
h800-l05:      5: ParallelTransformerLayerPipe
h800-l05:      6: ParallelTransformerLayerPipe
h800-l05:      7: ParallelTransformerLayerPipe
h800-l05:      8: ParallelTransformerLayerPipe
h800-l05:      9: ParallelTransformerLayerPipe
h800-l05:     10: ParallelTransformerLayerPipe
h800-l05:     11: ParallelTransformerLayerPipe
h800-l05: stage=1 layers=10
h800-l05:     12: ParallelTransformerLayerPipe
h800-l05:     13: ParallelTransformerLayerPipe
h800-l05:     14: ParallelTransformerLayerPipe
h800-l05:     15: ParallelTransformerLayerPipe
h800-l05:     16: ParallelTransformerLayerPipe
h800-l05:     17: ParallelTransformerLayerPipe
h800-l05:     18: ParallelTransformerLayerPipe
h800-l05:     19: ParallelTransformerLayerPipe
h800-l05:     20: ParallelTransformerLayerPipe
h800-l05:     21: ParallelTransformerLayerPipe
h800-l05: stage=2 layers=10
h800-l05:     22: ParallelTransformerLayerPipe
h800-l05:     23: ParallelTransformerLayerPipe
h800-l05:     24: ParallelTransformerLayerPipe
h800-l05:     25: ParallelTransformerLayerPipe
h800-l05:     26: ParallelTransformerLayerPipe
h800-l05:     27: ParallelTransformerLayerPipe
h800-l05:     28: ParallelTransformerLayerPipe
h800-l05:     29: ParallelTransformerLayerPipe
h800-l05:     30: ParallelTransformerLayerPipe
h800-l05:     31: ParallelTransformerLayerPipe
h800-l05: stage=3 layers=10
h800-l05:     32: ParallelTransformerLayerPipe
h800-l05:     33: ParallelTransformerLayerPipe
h800-l05:     34: ParallelTransformerLayerPipe
h800-l05:     35: ParallelTransformerLayerPipe
h800-l05:     36: ParallelTransformerLayerPipe
h800-l05:     37: ParallelTransformerLayerPipe
h800-l05:     38: ParallelTransformerLayerPipe
h800-l05:     39: ParallelTransformerLayerPipe
h800-l05:     40: ParallelTransformerLayerPipe
h800-l05:     41: ParallelTransformerLayerPipe
h800-l05: stage=4 layers=10
h800-l05:     42: ParallelTransformerLayerPipe
h800-l05:     43: ParallelTransformerLayerPipe
h800-l05:     44: ParallelTransformerLayerPipe
h800-l05:     45: ParallelTransformerLayerPipe
h800-l05:     46: ParallelTransformerLayerPipe
h800-l05:     47: ParallelTransformerLayerPipe
h800-l05:     48: ParallelTransformerLayerPipe
h800-l05:     49: ParallelTransformerLayerPipe
h800-l05:     50: ParallelTransformerLayerPipe
h800-l05:     51: ParallelTransformerLayerPipe
h800-l05: stage=5 layers=10
h800-l05:     52: ParallelTransformerLayerPipe
h800-l05:     53: ParallelTransformerLayerPipe
h800-l05:     54: ParallelTransformerLayerPipe
h800-l05:     55: ParallelTransformerLayerPipe
h800-l05:     56: ParallelTransformerLayerPipe
h800-l05:     57: ParallelTransformerLayerPipe
h800-l05:     58: ParallelTransformerLayerPipe
h800-l05:     59: ParallelTransformerLayerPipe
h800-l05:     60: ParallelTransformerLayerPipe
h800-l05:     61: ParallelTransformerLayerPipe
h800-l05: stage=6 layers=10
h800-l05:     62: ParallelTransformerLayerPipe
h800-l05:     63: ParallelTransformerLayerPipe
h800-l05:     64: ParallelTransformerLayerPipe
h800-l05:     65: ParallelTransformerLayerPipe
h800-l05:     66: ParallelTransformerLayerPipe
h800-l05:     67: ParallelTransformerLayerPipe
h800-l05:     68: ParallelTransformerLayerPipe
h800-l05:     69: ParallelTransformerLayerPipe
h800-l05:     70: ParallelTransformerLayerPipe
h800-l05:     71: ParallelTransformerLayerPipe
h800-l05: stage=7 layers=13
h800-l05:     72: ParallelTransformerLayerPipe
h800-l05:     73: ParallelTransformerLayerPipe
h800-l05:     74: ParallelTransformerLayerPipe
h800-l05:     75: ParallelTransformerLayerPipe
h800-l05:     76: ParallelTransformerLayerPipe
h800-l05:     77: ParallelTransformerLayerPipe
h800-l05:     78: ParallelTransformerLayerPipe
h800-l05:     79: ParallelTransformerLayerPipe
h800-l05:     80: ParallelTransformerLayerPipe
h800-l05:     81: ParallelTransformerLayerPipe
h800-l05:     82: MixedFusedLayerNorm
h800-l05:     83: EmbeddingPipe
h800-l05:     84: float16_to_fp32
h800-l05:   loss: loss_func
h800-l05:  > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 8054128640
h800-l07:  > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 8054128640
h800-l09:  > number of parameters on (tensor, pipeline) model parallel rank (0, 4): 8054128640
h800-l09:  > number of parameters on (tensor, pipeline) model parallel rank (0, 5): 8054128640
h800-l07:  > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 8054128640
h800-l03:  > number of parameters on (tensor, pipeline) model parallel rank (0, 6): 8054128640
h800-l05: [2024-05-08 16:10:17,984] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs
h800-l05: [2024-05-08 16:10:18,039] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs
h800-l05: [2024-05-08 16:10:18,040] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs
h800-l05: [2024-05-08 16:10:18,041] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs
h800-l07: [2024-05-08 16:10:18,198] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs
h800-l07: [2024-05-08 16:10:18,202] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs
h800-l09: [2024-05-08 16:10:18,191] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs
h800-l07: [2024-05-08 16:10:18,225] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs
h800-l09: [2024-05-08 16:10:18,208] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs
h800-l03:  > number of parameters on (tensor, pipeline) model parallel rank (0, 7): 8646590464
h800-l07: [2024-05-08 16:10:18,294] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs
h800-l09: [2024-05-08 16:10:18,272] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs
h800-l07: [2024-05-08 16:10:18,317] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs
h800-l07: [2024-05-08 16:10:18,322] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs
h800-l07: [2024-05-08 16:10:18,323] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs
h800-l07: [2024-05-08 16:10:18,323] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs
h800-l09: [2024-05-08 16:10:18,298] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs
h800-l05: [2024-05-08 16:10:18,287] [INFO] [utils.py:803:see_memory_usage] After Building Model
h800-l05: [2024-05-08 16:10:18,288] [INFO] [utils.py:804:see_memory_usage] MA 16.42 GB         Max_MA 16.73 GB         CA 16.73 GB         Max_CA 17 GB 
h800-l05: [2024-05-08 16:10:18,288] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 29.02 GB, percent = 1.9%
h800-l05:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 8646574080
h800-l09: [2024-05-08 16:10:18,314] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs
h800-l09: [2024-05-08 16:10:18,316] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs
h800-l09: [2024-05-08 16:10:18,318] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs
h800-l09: [2024-05-08 16:10:18,318] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs
h800-l03: [2024-05-08 16:10:18,682] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs
h800-l03: [2024-05-08 16:10:18,682] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs
h800-l03: [2024-05-08 16:10:18,695] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs
h800-l03: [2024-05-08 16:10:18,702] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs
h800-l05: [2024-05-08 16:10:19,391] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs
h800-l03: [2024-05-08 16:10:19,482] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs
h800-l05: [2024-05-08 16:10:19,436] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs
h800-l05: [2024-05-08 16:10:19,438] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs
h800-l03: [2024-05-08 16:10:19,485] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs
h800-l03: [2024-05-08 16:10:19,496] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs
h800-l03: [2024-05-08 16:10:19,503] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs
h800-l05: [2024-05-08 16:10:19,478] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs
h800-l05: Using /home/cestc/.cache/torch_extensions/py38_cu122 as PyTorch extensions root...
h800-l05: Detected CUDA files, patching ldflags
h800-l05: Emitting ninja build file /home/cestc/.cache/torch_extensions/py38_cu122/cpu_adam/build.ninja...
h800-l05: Building extension module cpu_adam...
h800-l05: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
h800-l05: Using /home/cestc/.cache/torch_extensions/py38_cu122 as PyTorch extensions root...
h800-l05: ninja: no work to do.
h800-l05: Loading extension module cpu_adam...
h800-l05: Time to load cpu_adam op: 2.522862672805786 seconds
h800-l05: Using /home/cestc/.cache/torch_extensions/py38_cu122 as PyTorch extensions root...
h800-l05: Using /home/cestc/.cache/torch_extensions/py38_cu122 as PyTorch extensions root...
h800-l05: Detected CUDA files, patching ldflags
h800-l05: Emitting ninja build file /home/cestc/.cache/torch_extensions/py38_cu122/cpu_adam/build.ninja...
h800-l05: Building extension module cpu_adam...
h800-l05: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
h800-l05: ninja: no work to do.
h800-l05: Loading extension module cpu_adam...
h800-l05: Time to load cpu_adam op: 2.5286314487457275 seconds
h800-l05: Loading extension module cpu_adam...
h800-l05: Time to load cpu_adam op: 2.5634384155273438 seconds
h800-l05: Loading extension module cpu_adam...
h800-l05: Time to load cpu_adam op: 2.583576202392578 seconds
h800-l09: Using /home/cestc/.cache/torch_extensions/py38_cu122 as PyTorch extensions root...
h800-l09: Using /home/cestc/.cache/torch_extensions/py38_cu122 as PyTorch extensions root...
h800-l09: Detected CUDA files, patching ldflags
h800-l09: Emitting ninja build file /home/cestc/.cache/torch_extensions/py38_cu122/cpu_adam/build.ninja...
h800-l09: Building extension module cpu_adam...
h800-l09: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
h800-l07: Using /home/cestc/.cache/torch_extensions/py38_cu122 as PyTorch extensions root...
h800-l09: ninja: no work to do.
h800-l09: Loading extension module cpu_adam...
h800-l07: Using /home/cestc/.cache/torch_extensions/py38_cu122 as PyTorch extensions root...
h800-l09: Time to load cpu_adam op: 2.5579261779785156 seconds
h800-l07: Using /home/cestc/.cache/torch_extensions/py38_cu122 as PyTorch extensions root...
h800-l07: Detected CUDA files, patching ldflags
h800-l07: Emitting ninja build file /home/cestc/.cache/torch_extensions/py38_cu122/cpu_adam/build.ninja...
h800-l07: Building extension module cpu_adam...
h800-l07: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
h800-l09: Using /home/cestc/.cache/torch_extensions/py38_cu122 as PyTorch extensions root...
h800-l07: ninja: no work to do.
h800-l07: Loading extension module cpu_adam...
h800-l07: Time to load cpu_adam op: 2.6117866039276123 seconds
h800-l09: Using /home/cestc/.cache/torch_extensions/py38_cu122 as PyTorch extensions root...
h800-l09: Detected CUDA files, patching ldflags
h800-l09: Emitting ninja build file /home/cestc/.cache/torch_extensions/py38_cu122/cpu_adam/build.ninja...
h800-l09: Building extension module cpu_adam...
h800-l09: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
h800-l05: Adam Optimizer #0 is created with AVX512 arithmetic capability.
h800-l05: Config: alpha=0.000200, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
h800-l05: [2024-05-08 16:10:20,823] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l09: Using /home/cestc/.cache/torch_extensions/py38_cu122 as PyTorch extensions root...
h800-l09: ninja: no work to do.
h800-l09: Loading extension module cpu_adam...
h800-l09: Time to load cpu_adam op: 2.5566771030426025 seconds
h800-l07: Loading extension module cpu_adam...
h800-l07: Loading extension module cpu_adam...
h800-l07: Time to load cpu_adam op: 2.672346353530884 seconds
h800-l07: Time to load cpu_adam op: 2.647614002227783 seconds
h800-l07: Using /home/cestc/.cache/torch_extensions/py38_cu122 as PyTorch extensions root...
h800-l09: Using /home/cestc/.cache/torch_extensions/py38_cu122 as PyTorch extensions root...
h800-l07: Using /home/cestc/.cache/torch_extensions/py38_cu122 as PyTorch extensions root...
h800-l05: Adam Optimizer #0 is created with AVX512 arithmetic capability.
h800-l05: Config: alpha=0.000200, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
h800-l05: [2024-05-08 16:10:20,871] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l07: Detected CUDA files, patching ldflags
h800-l07: Emitting ninja build file /home/cestc/.cache/torch_extensions/py38_cu122/cpu_adam/build.ninja...
h800-l07: Building extension module cpu_adam...
h800-l07: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
h800-l09: Detected CUDA files, patching ldflags
h800-l09: Emitting ninja build file /home/cestc/.cache/torch_extensions/py38_cu122/cpu_adam/build.ninja...
h800-l09: Building extension module cpu_adam...
h800-l09: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
h800-l09: Using /home/cestc/.cache/torch_extensions/py38_cu122 as PyTorch extensions root...
h800-l09: Using /home/cestc/.cache/torch_extensions/py38_cu122 as PyTorch extensions root...
h800-l07: Using /home/cestc/.cache/torch_extensions/py38_cu122 as PyTorch extensions root...
h800-l07: Using /home/cestc/.cache/torch_extensions/py38_cu122 as PyTorch extensions root...
h800-l07: Using /home/cestc/.cache/torch_extensions/py38_cu122 as PyTorch extensions root...
h800-l05: Adam Optimizer #0 is created with AVX512 arithmetic capability.
h800-l05: Config: alpha=0.000200, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
h800-l05: [2024-05-08 16:10:20,929] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l09: ninja: no work to do.
h800-l09: Loading extension module cpu_adam...
h800-l07: ninja: no work to do.
h800-l07: Loading extension module cpu_adam...
h800-l09: Time to load cpu_adam op: 2.596424102783203 seconds
h800-l07: Time to load cpu_adam op: 2.616267681121826 seconds
h800-l05: Adam Optimizer #0 is created with AVX512 arithmetic capability.
h800-l05: Config: alpha=0.000200, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
h800-l05: [2024-05-08 16:10:20,936] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l09: Loading extension module cpu_adam...
h800-l09: Time to load cpu_adam op: 2.603060007095337 seconds
h800-l07: Loading extension module cpu_adam...
h800-l07: Time to load cpu_adam op: 2.6851015090942383 seconds
h800-l09: Loading extension module cpu_adam...
h800-l09: Time to load cpu_adam op: 2.6874921321868896 seconds
h800-l09: Adam Optimizer #0 is created with AVX512 arithmetic capability.
h800-l09: Config: alpha=0.000200, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
h800-l09: [2024-05-08 16:10:21,022] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l09: Loading extension module cpu_adam...
h800-l09: Time to load cpu_adam op: 2.6585776805877686 seconds
h800-l09: Loading extension module cpu_adam...
h800-l09: Time to load cpu_adam op: 2.6621968746185303 seconds
h800-l07: Loading extension module cpu_adam...
h800-l07: Time to load cpu_adam op: 2.6886465549468994 seconds
h800-l09: Loading extension module cpu_adam...
h800-l09: Time to load cpu_adam op: 2.793740749359131 seconds
h800-l07: Loading extension module cpu_adam...
h800-l07: Time to load cpu_adam op: 2.709197521209717 seconds
h800-l07: Loading extension module cpu_adam...
h800-l07: Time to load cpu_adam op: 2.719247817993164 seconds
h800-l07: Adam Optimizer #0 is created with AVX512 arithmetic capability.
h800-l07: Config: alpha=0.000200, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
h800-l07: [2024-05-08 16:10:21,091] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l03: Using /home/cestc/.cache/torch_extensions/py38_cu122 as PyTorch extensions root...
h800-l09: Adam Optimizer #0 is created with AVX512 arithmetic capability.
h800-l09: Config: alpha=0.000200, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
h800-l09: [2024-05-08 16:10:21,184] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l03: Detected CUDA files, patching ldflags
h800-l03: Emitting ninja build file /home/cestc/.cache/torch_extensions/py38_cu122/cpu_adam/build.ninja...
h800-l03: Building extension module cpu_adam...
h800-l03: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
h800-l03: Using /home/cestc/.cache/torch_extensions/py38_cu122 as PyTorch extensions root...
h800-l03: ninja: no work to do.
h800-l03: Loading extension module cpu_adam...
h800-l03: Time to load cpu_adam op: 2.530905246734619 seconds
h800-l07: Adam Optimizer #0 is created with AVX512 arithmetic capability.
h800-l07: Config: alpha=0.000200, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
h800-l07: [2024-05-08 16:10:21,269] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l03: Using /home/cestc/.cache/torch_extensions/py38_cu122 as PyTorch extensions root...
h800-l09: Adam Optimizer #0 is created with AVX512 arithmetic capability.
h800-l09: Config: alpha=0.000200, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
h800-l03: Using /home/cestc/.cache/torch_extensions/py38_cu122 as PyTorch extensions root...
h800-l09: [2024-05-08 16:10:21,252] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l07: Adam Optimizer #0 is created with AVX512 arithmetic capability.
h800-l07: Config: alpha=0.000200, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
h800-l07: [2024-05-08 16:10:21,282] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l09: Adam Optimizer #0 is created with AVX512 arithmetic capability.
h800-l09: Config: alpha=0.000200, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
h800-l09: [2024-05-08 16:10:21,257] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l03: Detected CUDA files, patching ldflags
h800-l03: Emitting ninja build file /home/cestc/.cache/torch_extensions/py38_cu122/cpu_adam/build.ninja...
h800-l03: Building extension module cpu_adam...
h800-l03: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
h800-l03: ninja: no work to do.
h800-l03: Loading extension module cpu_adam...
h800-l03: Time to load cpu_adam op: 2.603931188583374 seconds
h800-l07: Adam Optimizer #0 is created with AVX512 arithmetic capability.
h800-l07: Config: alpha=0.000200, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
h800-l07: Adam Optimizer #0 is created with AVX512 arithmetic capability.
h800-l07: Config: alpha=0.000200, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
h800-l03: Loading extension module cpu_adam...
h800-l07: [2024-05-08 16:10:21,344] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l07: [2024-05-08 16:10:21,344] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l03: Time to load cpu_adam op: 2.601863384246826 seconds
h800-l07: Adam Optimizer #0 is created with AVX512 arithmetic capability.
h800-l07: Config: alpha=0.000200, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
h800-l07: [2024-05-08 16:10:21,373] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l03: Loading extension module cpu_adam...
h800-l03: Time to load cpu_adam op: 2.6612751483917236 seconds
h800-l09: Adam Optimizer #0 is created with AVX512 arithmetic capability.
h800-l09: Config: alpha=0.000200, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
h800-l09: [2024-05-08 16:10:21,376] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l07: Adam Optimizer #0 is created with AVX512 arithmetic capability.
h800-l07: Config: alpha=0.000200, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
h800-l07: [2024-05-08 16:10:21,414] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l07: Adam Optimizer #0 is created with AVX512 arithmetic capability.
h800-l07: Config: alpha=0.000200, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
h800-l07: [2024-05-08 16:10:21,416] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l09: Adam Optimizer #0 is created with AVX512 arithmetic capability.
h800-l09: Config: alpha=0.000200, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
h800-l09: [2024-05-08 16:10:21,394] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l09: Adam Optimizer #0 is created with AVX512 arithmetic capability.
h800-l09: Config: alpha=0.000200, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
h800-l09: [2024-05-08 16:10:21,398] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l09: Adam Optimizer #0 is created with AVX512 arithmetic capability.
h800-l09: Config: alpha=0.000200, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
h800-l09: [2024-05-08 16:10:21,402] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l03: Adam Optimizer #0 is created with AVX512 arithmetic capability.
h800-l03: Config: alpha=0.000200, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
h800-l03: [2024-05-08 16:10:21,512] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l03: Adam Optimizer #0 is created with AVX512 arithmetic capability.
h800-l03: Config: alpha=0.000200, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
h800-l03: [2024-05-08 16:10:21,656] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l03: Adam Optimizer #0 is created with AVX512 arithmetic capability.
h800-l03: Config: alpha=0.000200, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
h800-l03: [2024-05-08 16:10:21,664] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l03: Adam Optimizer #0 is created with AVX512 arithmetic capability.
h800-l03: Config: alpha=0.000200, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
h800-l03: [2024-05-08 16:10:21,669] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l05: Using /home/cestc/.cache/torch_extensions/py38_cu122 as PyTorch extensions root...
h800-l05: Detected CUDA files, patching ldflags
h800-l05: Emitting ninja build file /home/cestc/.cache/torch_extensions/py38_cu122/cpu_adam/build.ninja...
h800-l05: Building extension module cpu_adam...
h800-l05: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
h800-l05: Using /home/cestc/.cache/torch_extensions/py38_cu122 as PyTorch extensions root...
h800-l05: ninja: no work to do.
h800-l05: Loading extension module cpu_adam...
h800-l05: Time to load cpu_adam op: 2.5196385383605957 seconds
h800-l03: Using /home/cestc/.cache/torch_extensions/py38_cu122 as PyTorch extensions root...
h800-l05: Using /home/cestc/.cache/torch_extensions/py38_cu122 as PyTorch extensions root...
h800-l03: Detected CUDA files, patching ldflags
h800-l03: Emitting ninja build file /home/cestc/.cache/torch_extensions/py38_cu122/cpu_adam/build.ninja...
h800-l03: Building extension module cpu_adam...
h800-l03: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
h800-l05: Using /home/cestc/.cache/torch_extensions/py38_cu122 as PyTorch extensions root...
h800-l05: Detected CUDA files, patching ldflags
h800-l05: Emitting ninja build file /home/cestc/.cache/torch_extensions/py38_cu122/cpu_adam/build.ninja...
h800-l05: Building extension module cpu_adam...
h800-l05: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
h800-l03: ninja: no work to do.
h800-l03: Loading extension module cpu_adam...
h800-l03: Time to load cpu_adam op: 2.536271095275879 seconds
h800-l05: ninja: no work to do.
h800-l05: Loading extension module cpu_adam...
h800-l05: Time to load cpu_adam op: 2.5487842559814453 seconds
h800-l03: Using /home/cestc/.cache/torch_extensions/py38_cu122 as PyTorch extensions root...
h800-l03: Using /home/cestc/.cache/torch_extensions/py38_cu122 as PyTorch extensions root...
h800-l03: Detected CUDA files, patching ldflags
h800-l03: Emitting ninja build file /home/cestc/.cache/torch_extensions/py38_cu122/cpu_adam/build.ninja...
h800-l03: Building extension module cpu_adam...
h800-l03: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
h800-l03: Using /home/cestc/.cache/torch_extensions/py38_cu122 as PyTorch extensions root...
h800-l05: Loading extension module cpu_adam...
h800-l05: Time to load cpu_adam op: 2.5717978477478027 seconds
h800-l03: ninja: no work to do.
h800-l03: Loading extension module cpu_adam...
h800-l03: Time to load cpu_adam op: 2.62451434135437 seconds
h800-l05: Loading extension module cpu_adam...
h800-l05: Time to load cpu_adam op: 2.648465156555176 seconds
h800-l03: Loading extension module cpu_adam...
h800-l03: Time to load cpu_adam op: 2.685758352279663 seconds
h800-l03: Loading extension module cpu_adam...
h800-l03: Time to load cpu_adam op: 2.7097253799438477 seconds
h800-l05: Adam Optimizer #0 is created with AVX512 arithmetic capability.
h800-l05: Config: alpha=0.000200, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
h800-l05: [2024-05-08 16:10:22,418] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l05: Adam Optimizer #0 is created with AVX512 arithmetic capability.
h800-l05: Config: alpha=0.000200, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
h800-l05: [2024-05-08 16:10:22,555] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l05: Adam Optimizer #0 is created with AVX512 arithmetic capability.
h800-l05: Config: alpha=0.000200, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
h800-l05: Adam Optimizer #0 is created with AVX512 arithmetic capability.
h800-l05: Config: alpha=0.000200, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
h800-l05: > learning rate decay style: cosine
h800-l05: DeepSpeed is enabled.
h800-l05: [2024-05-08 16:10:22,637] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.3+04a6fedf, git-hash=04a6fedf, git-branch=HEAD
h800-l05: [2024-05-08 16:10:22,637] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l05: [2024-05-08 16:10:22,637] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l03: Adam Optimizer #0 is created with AVX512 arithmetic capability.
h800-l03: Config: alpha=0.000200, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
h800-l03: [2024-05-08 16:10:23,077] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l03: Adam Optimizer #0 is created with AVX512 arithmetic capability.
h800-l03: Config: alpha=0.000200, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
h800-l03: [2024-05-08 16:10:23,081] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l03: Adam Optimizer #0 is created with AVX512 arithmetic capability.
h800-l03: Config: alpha=0.000200, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
h800-l03: [2024-05-08 16:10:23,146] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l03: Adam Optimizer #0 is created with AVX512 arithmetic capability.
h800-l03: Config: alpha=0.000200, betas=(0.900000, 0.950000), weight_decay=0.100000, adam_w=1
h800-l03: [2024-05-08 16:10:23,154] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
h800-l05: [2024-05-08 16:10:24,213] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
h800-l05: [2024-05-08 16:10:24,214] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
h800-l05: [2024-05-08 16:10:24,214] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
h800-l05: [2024-05-08 16:10:24,217] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
h800-l05: [2024-05-08 16:10:24,217] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
h800-l05: [2024-05-08 16:10:24,217] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 1 optimizer
h800-l05: [2024-05-08 16:10:24,217] [INFO] [stage_1_and_2.py:146:__init__] Reduce bucket size 50000000
h800-l05: [2024-05-08 16:10:24,217] [INFO] [stage_1_and_2.py:147:__init__] Allgather bucket size 50000000
h800-l05: [2024-05-08 16:10:24,217] [INFO] [stage_1_and_2.py:148:__init__] CPU Offload: True
h800-l05: [2024-05-08 16:10:24,217] [INFO] [stage_1_and_2.py:149:__init__] Round robin gradient partitioning: False
h800-l05: Rank: 4 partition count [4, 4] and sizes[(2013265920, False), (266240, False)] 
h800-l07: Rank: 15 partition count [4, 4] and sizes[(2013265920, False), (266240, False)] 
h800-l05: Rank: 6 partition count [4, 4] and sizes[(2013265920, False), (266240, False)] 
h800-l09: Rank: 16 partition count [4, 4] and sizes[(2013265920, False), (266240, False)] 
h800-l05: Rank: 7 partition count [4, 4] and sizes[(2013265920, False), (266240, False)] 
h800-l07: Rank: 10 partition count [4, 4] and sizes[(2013265920, False), (266240, False)] 
h800-l05: Rank: 3 partition count [4, 4] and sizes[(2161377280, False), (266240, False)] 
h800-l05: Rank: 1 partition count [4, 4] and sizes[(2161377280, False), (266240, False)] 
h800-l03: Rank: 27 partition count [4, 4] and sizes[(2013265920, False), (266240, False)] 
h800-l03: Rank: 25 partition count [4, 4] and sizes[(2013265920, False), (266240, False)] 
h800-l03: Rank: 26 partition count [4, 4] and sizes[(2013265920, False), (266240, False)] 
h800-l09: Rank: 20 partition count [4, 4] and sizes[(2013265920, False), (266240, False)] 
h800-l03: Rank: 24 partition count [4, 4] and sizes[(2013265920, False), (266240, False)] 
h800-l09: Rank: 18 partition count [4, 4] and sizes[(2013265920, False), (266240, False)] 
h800-l09: Rank: 19 partition count [4, 4] and sizes[(2013265920, False), (266240, False)] 
h800-l09: Rank: 21 partition count [4, 4] and sizes[(2013265920, False), (266240, False)] 
h800-l05: Rank: 2 partition count [4, 4] and sizes[(2161377280, False), (266240, False)] 
h800-l09: Rank: 23 partition count [4, 4] and sizes[(2013265920, False), (266240, False)] 
h800-l03: Rank: 31 partition count [4, 4] and sizes[(2161377280, False), (270336, False)] 
h800-l09: Rank: 17 partition count [4, 4] and sizes[(2013265920, False), (266240, False)] 
h800-l07: Rank: 8 partition count [4, 4] and sizes[(2013265920, False), (266240, False)] 
h800-l07: Rank: 9 partition count [4, 4] and sizes[(2013265920, False), (266240, False)] 
h800-l03: Rank: 28 partition count [4, 4] and sizes[(2161377280, False), (270336, False)] 
h800-l07: Rank: 14 partition count [4, 4] and sizes[(2013265920, False), (266240, False)] 
h800-l07: Rank: 13 partition count [4, 4] and sizes[(2013265920, False), (266240, False)] 
h800-l07: Rank: 12 partition count [4, 4] and sizes[(2013265920, False), (266240, False)] 
h800-l03: Rank: 30 partition count [4, 4] and sizes[(2161377280, False), (270336, False)] 
h800-l05: Rank: 0 partition count [4, 4] and sizes[(2161377280, False), (266240, False)] 
h800-l05: Rank: 5 partition count [4, 4] and sizes[(2013265920, False), (266240, False)] 
h800-l07: Rank: 11 partition count [4, 4] and sizes[(2013265920, False), (266240, False)] 
h800-l09: Rank: 22 partition count [4, 4] and sizes[(2013265920, False), (266240, False)] 
h800-l03: Rank: 29 partition count [4, 4] and sizes[(2161377280, False), (270336, False)] 
h800-l05: [2024-05-08 16:10:45,397] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states
h800-l05: [2024-05-08 16:10:45,397] [INFO] [utils.py:804:see_memory_usage] MA 17.15 GB         Max_MA 17.15 GB         CA 17.33 GB         Max_CA 17 GB 
h800-l05: [2024-05-08 16:10:45,398] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 142.5 GB, percent = 9.4%
h800-l05: [2024-05-08 16:10:49,365] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states
h800-l05: [2024-05-08 16:10:49,365] [INFO] [utils.py:804:see_memory_usage] MA 17.15 GB         Max_MA 17.15 GB         CA 17.33 GB         Max_CA 17 GB 
h800-l05: [2024-05-08 16:10:49,366] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 293.24 GB, percent = 19.4%
h800-l05: [2024-05-08 16:10:49,366] [INFO] [stage_1_and_2.py:520:__init__] optimizer state initialized
h800-l05: [2024-05-08 16:10:49,423] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer
h800-l05: [2024-05-08 16:10:49,423] [INFO] [utils.py:804:see_memory_usage] MA 17.15 GB         Max_MA 17.15 GB         CA 17.33 GB         Max_CA 17 GB 
h800-l05: [2024-05-08 16:10:49,424] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 293.29 GB, percent = 19.4%
h800-l05: [2024-05-08 16:10:49,425] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
h800-l05: [2024-05-08 16:10:49,425] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
h800-l05: [2024-05-08 16:10:49,425] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.optimizer_param_scheduler.OptimizerParamScheduler object at 0x7f60fbd099d0>
h800-l05: [2024-05-08 16:10:49,425] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.95), (0.9, 0.95)]
h800-l05: [2024-05-08 16:10:49,425] [INFO] [config.py:967:print] DeepSpeedEngine configuration:
h800-l05: [2024-05-08 16:10:49,425] [INFO] [config.py:971:print]   activation_checkpointing_config  {
h800-l05:     "partition_activations": false, 
h800-l05:     "contiguous_memory_optimization": false, 
h800-l05:     "cpu_checkpointing": false, 
h800-l05:     "number_checkpoints": null, 
h800-l05:     "synchronize_checkpoint_boundary": false, 
h800-l05:     "profile": false
h800-l05: }
h800-l05: [2024-05-08 16:10:49,425] [INFO] [config.py:971:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
h800-l05: [2024-05-08 16:10:49,425] [INFO] [config.py:971:print]   amp_enabled .................. False
h800-l05: [2024-05-08 16:10:49,425] [INFO] [config.py:971:print]   amp_params ................... False
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   autotuning_config ............ {
h800-l05:     "enabled": false, 
h800-l05:     "start_step": null, 
h800-l05:     "end_step": null, 
h800-l05:     "metric_path": null, 
h800-l05:     "arg_mappings": null, 
h800-l05:     "metric": "throughput", 
h800-l05:     "model_info": null, 
h800-l05:     "results_dir": "autotuning_results", 
h800-l05:     "exps_dir": "autotuning_exps", 
h800-l05:     "overwrite": true, 
h800-l05:     "fast": true, 
h800-l05:     "start_profile_step": 3, 
h800-l05:     "end_profile_step": 5, 
h800-l05:     "tuner_type": "gridsearch", 
h800-l05:     "tuner_early_stopping": 5, 
h800-l05:     "tuner_num_trials": 50, 
h800-l05:     "model_info_path": null, 
h800-l05:     "mp_size": 1, 
h800-l05:     "max_train_batch_size": null, 
h800-l05:     "min_train_batch_size": 1, 
h800-l05:     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
h800-l05:     "min_train_micro_batch_size_per_gpu": 1, 
h800-l05:     "num_tuning_micro_batch_sizes": 3
h800-l05: }
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   bfloat16_enabled ............. False
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   checkpoint_parallel_write_pipeline  False
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   checkpoint_tag_validation_enabled  True
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   checkpoint_tag_validation_fail  False
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f60fb987cd0>
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   communication_data_type ...... None
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   curriculum_enabled_legacy .... False
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   curriculum_params_legacy ..... False
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   data_efficiency_enabled ...... False
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   dataloader_drop_last ......... False
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   disable_allgather ............ False
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   dump_state ................... False
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   dynamic_loss_scale_args ...... {'init_scale': 16384, 'scale_window': 50000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1024}
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   eigenvalue_enabled ........... False
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   eigenvalue_gas_boundary_resolution  1
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   eigenvalue_layer_name ........ bert.encoder.layer
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   eigenvalue_layer_num ......... 0
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   eigenvalue_max_iter .......... 100
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   eigenvalue_stability ......... 1e-06
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   eigenvalue_tol ............... 0.01
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   eigenvalue_verbose ........... False
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   elasticity_enabled ........... False
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   flops_profiler_config ........ {
h800-l05:     "enabled": false, 
h800-l05:     "recompute_fwd_factor": 0.0, 
h800-l05:     "profile_step": 1, 
h800-l05:     "module_depth": -1, 
h800-l05:     "top_modules": 1, 
h800-l05:     "detailed": true, 
h800-l05:     "output_file": null
h800-l05: }
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   fp16_auto_cast ............... False
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   fp16_enabled ................. True
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   fp16_master_weights_and_gradients  False
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   global_rank .................. 0
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   grad_accum_dtype ............. None
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   gradient_accumulation_steps .. 32
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   gradient_clipping ............ 1.0
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   gradient_predivide_factor .... 1.0
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   initial_dynamic_scale ........ 16384
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   load_universal_checkpoint .... False
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   loss_scale ................... 4096
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   memory_breakdown ............. False
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   mics_hierarchial_params_gather  False
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   mics_shard_size .............. -1
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   nebula_config ................ {
h800-l05:     "enabled": false, 
h800-l05:     "persistent_storage_path": null, 
h800-l05:     "persistent_time_interval": 100, 
h800-l05:     "num_of_version_in_retention": 2, 
h800-l05:     "enable_nebula_load": true, 
h800-l05:     "load_path": null
h800-l05: }
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   optimizer_legacy_fusion ...... False
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   optimizer_name ............... None
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   optimizer_params ............. None
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   pld_enabled .................. False
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   pld_params ................... False
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   prescale_gradients ........... False
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   scheduler_name ............... None
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   scheduler_params ............. None
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   sparse_attention ............. None
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   sparse_gradients_enabled ..... False
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   steps_per_print .............. 1
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   train_batch_size ............. 128
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   train_micro_batch_size_per_gpu  1
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   use_node_local_storage ....... False
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   wall_clock_breakdown ......... True
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   weight_quantization_config ... None
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   world_size ................... 4
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   zero_allow_untested_optimizer  False
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   zero_config .................. stage=1 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=50000000 allgather_partitions=True allgather_bucket_size=50000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   zero_enabled ................. True
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   zero_force_ds_cpu_optimizer .. True
h800-l05: [2024-05-08 16:10:49,426] [INFO] [config.py:971:print]   zero_optimization_stage ...... 1
h800-l05: [2024-05-08 16:10:49,427] [INFO] [config.py:957:print_user_config]   json = {
h800-l05:     "train_batch_size": 128, 
h800-l05:     "train_micro_batch_size_per_gpu": 1, 
h800-l05:     "steps_per_print": 1, 
h800-l05:     "zero_optimization": {
h800-l05:         "stage": 1, 
h800-l05:         "contiguous_gradients": true, 
h800-l05:         "overlap_comm": true, 
h800-l05:         "reduce_scatter": true, 
h800-l05:         "reduce_bucket_size": 5.000000e+07, 
h800-l05:         "allgather_bucket_size": 5.000000e+07, 
h800-l05:         "cpu_offload": true
h800-l05:     }, 
h800-l05:     "gradient_clipping": 1.0, 
h800-l05:     "prescale_gradients": false, 
h800-l05:     "fp16": {
h800-l05:         "enabled": true, 
h800-l05:         "loss_scale": 4.096000e+03, 
h800-l05:         "loss_scale_window": 5.000000e+04, 
h800-l05:         "hysteresis": 2, 
h800-l05:         "min_loss_scale": 1.024000e+03, 
h800-l05:         "initial_scale_power": 14
h800-l05:     }, 
h800-l05:     "wall_clock_breakdown": true
h800-l05: }
h800-l05: [2024-05-08 16:10:49,427] [INFO] [engine.py:96:__init__] CONFIG: micro_batches=32 micro_batch_size=1
h800-l03: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l03: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l09: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l07: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l09: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l07: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l05: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l05: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l05: [2024-05-08 16:10:51,272] [INFO] [engine.py:151:__init__] RANK=4 STAGE=1 LAYERS=10 [12, 22) STAGE_PARAMS=8054128640 (8054.129M) TOTAL_PARAMS=65617936384 (65617.936M) UNIQUE_PARAMS=65025490944 (65025.491M)
h800-l05: [2024-05-08 16:10:51,272] [INFO] [engine.py:151:__init__] RANK=0 STAGE=0 LAYERS=12 [0, 12) STAGE_PARAMS=8646574080 (8646.574M) TOTAL_PARAMS=65617936384 (65617.936M) UNIQUE_PARAMS=65025490944 (65025.491M)
h800-l07: [2024-05-08 16:10:51,318] [INFO] [engine.py:151:__init__] RANK=12 STAGE=3 LAYERS=10 [32, 42) STAGE_PARAMS=8054128640 (8054.129M) TOTAL_PARAMS=65617936384 (65617.936M) UNIQUE_PARAMS=65025490944 (65025.491M)
h800-l09: [2024-05-08 16:10:51,293] [INFO] [engine.py:151:__init__] RANK=16 STAGE=4 LAYERS=10 [42, 52) STAGE_PARAMS=8054128640 (8054.129M) TOTAL_PARAMS=65617936384 (65617.936M) UNIQUE_PARAMS=65025490944 (65025.491M)
h800-l09: [2024-05-08 16:10:51,293] [INFO] [engine.py:151:__init__] RANK=20 STAGE=5 LAYERS=10 [52, 62) STAGE_PARAMS=8054128640 (8054.129M) TOTAL_PARAMS=65617936384 (65617.936M) UNIQUE_PARAMS=65025490944 (65025.491M)
h800-l03: [2024-05-08 16:10:51,319] [INFO] [engine.py:151:__init__] RANK=24 STAGE=6 LAYERS=10 [62, 72) STAGE_PARAMS=8054128640 (8054.129M) TOTAL_PARAMS=65617936384 (65617.936M) UNIQUE_PARAMS=65025490944 (65025.491M)
h800-l03: [2024-05-08 16:10:51,319] [INFO] [engine.py:151:__init__] RANK=28 STAGE=7 LAYERS=13 [72, 85) STAGE_PARAMS=8646590464 (8646.590M) TOTAL_PARAMS=65617936384 (65617.936M) UNIQUE_PARAMS=65025490944 (65025.491M)
h800-l07: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l05: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l05: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l03: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l03: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l09: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l09: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l07: [2024-05-08 16:10:51,320] [INFO] [engine.py:151:__init__] RANK=8 STAGE=2 LAYERS=10 [22, 32) STAGE_PARAMS=8054128640 (8054.129M) TOTAL_PARAMS=65617936384 (65617.936M) UNIQUE_PARAMS=65025490944 (65025.491M)
h800-l07: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l03: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l03: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l07: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l09: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l05: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l09: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l05: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l07: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l05: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l09: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l05: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l07: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l07: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l03: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l03: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l09: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l09: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l09: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l07: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l07: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l09: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l09: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l05: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l05: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l09: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l09: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l07: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l07: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l07: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l07: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l05: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l09: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l09: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l05: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l07: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l07: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l03: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l03: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l03: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l03: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l05: [2024-05-08 16:10:54,097] [WARNING] [engine.py:2681:load_checkpoint] Unable to find latest file at ./configs/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
h800-l05: WARNING: could not find the metadata file ./configs 
h800-l05:     will not load any checkpoints and will start from random
h800-l05: [2024-05-08 16:10:54,097] [WARNING] [engine.py:2681:load_checkpoint] Unable to find latest file at ./configs/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
h800-l05: [2024-05-08 16:10:54,097] [WARNING] [engine.py:2681:load_checkpoint] Unable to find latest file at ./configs/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
h800-l05: [2024-05-08 16:10:54,097] [WARNING] [engine.py:2681:load_checkpoint] Unable to find latest file at ./configs/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
h800-l05: [2024-05-08 16:10:54,097] [WARNING] [engine.py:2681:load_checkpoint] Unable to find latest file at ./configs/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
h800-l05: [2024-05-08 16:10:54,097] [WARNING] [engine.py:2681:load_checkpoint] Unable to find latest file at ./configs/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
h800-l05: [2024-05-08 16:10:54,097] [WARNING] [engine.py:2681:load_checkpoint] Unable to find latest file at ./configs/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
h800-l09: [2024-05-08 16:10:54,118] [WARNING] [engine.py:2681:load_checkpoint] Unable to find latest file at ./configs/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
h800-l03: [2024-05-08 16:10:54,144] [WARNING] [engine.py:2681:load_checkpoint] Unable to find latest file at ./configs/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
h800-l03: [2024-05-08 16:10:54,144] [WARNING] [engine.py:2681:load_checkpoint] Unable to find latest file at ./configs/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
h800-l09: [2024-05-08 16:10:54,118] [WARNING] [engine.py:2681:load_checkpoint] Unable to find latest file at ./configs/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
h800-l03: [2024-05-08 16:10:54,144] [WARNING] [engine.py:2681:load_checkpoint] Unable to find latest file at ./configs/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
h800-l07: [2024-05-08 16:10:54,143] [WARNING] [engine.py:2681:load_checkpoint] Unable to find latest file at ./configs/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
h800-l09: [2024-05-08 16:10:54,118] [WARNING] [engine.py:2681:load_checkpoint] Unable to find latest file at ./configs/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
h800-l07: [2024-05-08 16:10:54,143] [WARNING] [engine.py:2681:load_checkpoint] Unable to find latest file at ./configs/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
h800-l03: [2024-05-08 16:10:54,144] [WARNING] [engine.py:2681:load_checkpoint] Unable to find latest file at ./configs/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
h800-l03: [2024-05-08 16:10:54,144] [WARNING] [engine.py:2681:load_checkpoint] Unable to find latest file at ./configs/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
h800-l09: [2024-05-08 16:10:54,118] [WARNING] [engine.py:2681:load_checkpoint] Unable to find latest file at ./configs/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
h800-l05: [2024-05-08 16:10:54,097] [WARNING] [engine.py:2681:load_checkpoint] Unable to find latest file at ./configs/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
h800-l07: [2024-05-08 16:10:54,144] [WARNING] [engine.py:2681:load_checkpoint] Unable to find latest file at ./configs/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
h800-l03: [2024-05-08 16:10:54,144] [WARNING] [engine.py:2681:load_checkpoint] Unable to find latest file at ./configs/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
h800-l03: [2024-05-08 16:10:54,144] [WARNING] [engine.py:2681:load_checkpoint] Unable to find latest file at ./configs/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
h800-l03: [2024-05-08 16:10:54,144] [WARNING] [engine.py:2681:load_checkpoint] Unable to find latest file at ./configs/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
h800-l07: [2024-05-08 16:10:54,143] [WARNING] [engine.py:2681:load_checkpoint] Unable to find latest file at ./configs/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
h800-l09: [2024-05-08 16:10:54,118] [WARNING] [engine.py:2681:load_checkpoint] Unable to find latest file at ./configs/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
h800-l09: [2024-05-08 16:10:54,118] [WARNING] [engine.py:2681:load_checkpoint] Unable to find latest file at ./configs/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
h800-l09: [2024-05-08 16:10:54,118] [WARNING] [engine.py:2681:load_checkpoint] Unable to find latest file at ./configs/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
h800-l07: [2024-05-08 16:10:54,144] [WARNING] [engine.py:2681:load_checkpoint] Unable to find latest file at ./configs/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
h800-l07: [2024-05-08 16:10:54,144] [WARNING] [engine.py:2681:load_checkpoint] Unable to find latest file at ./configs/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
h800-l07: [2024-05-08 16:10:54,144] [WARNING] [engine.py:2681:load_checkpoint] Unable to find latest file at ./configs/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
h800-l07: [2024-05-08 16:10:54,144] [WARNING] [engine.py:2681:load_checkpoint] Unable to find latest file at ./configs/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
h800-l09: [2024-05-08 16:10:54,119] [WARNING] [engine.py:2681:load_checkpoint] Unable to find latest file at ./configs/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
h800-l03: (min, max) time across ranks (ms):
h800-l03:     load-checkpoint ................................: (1.52, 2.01)
h800-l05: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-05-08 16:10:54 
h800-l05: > building train, validation, and test datasets ...
h800-l05:  > datasets target sizes (minimum size):
h800-l05:     train:      25600000
h800-l05:     validation: 5120
h800-l05:     test:       5120
h800-l05: > building train, validation, and test datasets for GPT ...
h800-l05: Single data path provided for train, valid & test
h800-l05:  > building dataset index ...
h800-l05:     reading sizes...
h800-l05:     reading pointers...
h800-l05:     reading document index...
h800-l05:     creating numpy buffer of mmap...
h800-l05:     creating memory view of numpy buffer...
h800-l05:  > finished creating indexed dataset in 0.156011 seconds
h800-l05:     number of documents: 120000
h800-l05:  > dataset split:
h800-l05:     train:
h800-l05:      document indices in [0, 96000) total of 96000 documents
h800-l05:     validation:
h800-l05:      document indices in [96000, 108000) total of 12000 documents
h800-l05:     test:
h800-l05:      document indices in [108000, 120000) total of 12000 documents
h800-l05:  > WARNING: could not find index map files, building the indices on rank 0 ...
h800-l05:  > last epoch number of samples (2461) is smaller than 80% of number of samples per epoch (18015), setting separate_last_epoch to True
h800-l05:  > elasped time to build and save doc-idx mapping (seconds): 9.336001
h800-l05:     using:
h800-l05:      number of documents:       96000
h800-l05:      number of epochs:          1429
h800-l05:      sequence length:           8192
h800-l05:      total number of samples:   25743555
h800-l05:  > elasped time to build and save sample-idx mapping (seconds): 2.932438
h800-l05:  > building shuffle index with split [0, 25725539) and [25725539, 25743555) ...
h800-l05:  > elasped time to build and save shuffle-idx mapping (seconds): 1.430418
h800-l05:  > loading doc-idx mapping from /share/train_data/index-cache/309599dc48475e0edc39154208f84362_doc_idx.npy
h800-l05:  > loading sample-idx mapping from /share/train_data/index-cache/309599dc48475e0edc39154208f84362_sample_idx.npy
h800-l05:  > loading shuffle-idx mapping from /share/train_data/index-cache/309599dc48475e0edc39154208f84362_shuffle_idx.npy
h800-l05:     loaded indexed file in 0.759 seconds
h800-l05:     total number of samples: 25743556
h800-l05:     total number of epochs: 1429
h800-l05:  > WARNING: could not find index map files, building the indices on rank 0 ...
h800-l05:  > last epoch number of samples (1234) is smaller than 80% of number of samples per epoch (1956), setting separate_last_epoch to True
h800-l05:  > elasped time to build and save doc-idx mapping (seconds): 0.062512
h800-l05:     using:
h800-l05:      number of documents:       12000
h800-l05:      number of epochs:          3
h800-l05:      sequence length:           8192
h800-l05:      total number of samples:   5868
h800-l05:  > elasped time to build and save sample-idx mapping (seconds): 0.050494
h800-l05:  > building shuffle index with split [0, 3912) and [3912, 5868) ...
h800-l05:  > elasped time to build and save shuffle-idx mapping (seconds): 0.049379
h800-l05:  > loading doc-idx mapping from /share/train_data/index-cache/74189b0ed1662659c5519df700ed819e_doc_idx.npy
h800-l05:  > loading sample-idx mapping from /share/train_data/index-cache/74189b0ed1662659c5519df700ed819e_sample_idx.npy
h800-l05:  > loading shuffle-idx mapping from /share/train_data/index-cache/74189b0ed1662659c5519df700ed819e_shuffle_idx.npy
h800-l05:     loaded indexed file in 0.157 seconds
h800-l05:     total number of samples: 5869
h800-l05:     total number of epochs: 3
h800-l05:  > WARNING: could not find index map files, building the indices on rank 0 ...
h800-l05:  > last epoch number of samples (1087) is smaller than 80% of number of samples per epoch (2029), setting separate_last_epoch to True
h800-l05:  > elasped time to build and save doc-idx mapping (seconds): 0.055160
h800-l05:     using:
h800-l05:      number of documents:       12000
h800-l05:      number of epochs:          3
h800-l05:      sequence length:           8192
h800-l05:      total number of samples:   6089
h800-l05:  > elasped time to build and save sample-idx mapping (seconds): 0.061574
h800-l05:  > building shuffle index with split [0, 4059) and [4059, 6089) ...
h800-l05:  > elasped time to build and save shuffle-idx mapping (seconds): 0.052390
h800-l05:  > loading doc-idx mapping from /share/train_data/index-cache/395e3d0cc607369fdfd7797a9faed7c9_doc_idx.npy
h800-l05:  > loading sample-idx mapping from /share/train_data/index-cache/395e3d0cc607369fdfd7797a9faed7c9_sample_idx.npy
h800-l05:  > loading shuffle-idx mapping from /share/train_data/index-cache/395e3d0cc607369fdfd7797a9faed7c9_shuffle_idx.npy
h800-l05:     loaded indexed file in 0.177 seconds
h800-l05:     total number of samples: 6090
h800-l05:     total number of epochs: 3
h800-l05: > building indices for blendable datasets ...
h800-l05:  > sample ratios:
h800-l05:    dataset 0, input: 1, achieved: 1
h800-l05: > elapsed time for building blendable dataset indices: 0.08 (sec)
h800-l05: > size of blendable dataset: 25728000 samples
h800-l05: > building indices for blendable datasets ...
h800-l05:  > sample ratios:
h800-l05:    dataset 0, input: 1, achieved: 1
h800-l05: > elapsed time for building blendable dataset indices: 0.00 (sec)
h800-l05: > size of blendable dataset: 5146 samples
h800-l05: > building indices for blendable datasets ...
h800-l05:  > sample ratios:
h800-l05:    dataset 0, input: 1, achieved: 1
h800-l05: > elapsed time for building blendable dataset indices: 0.00 (sec)
h800-l05: > size of blendable dataset: 5146 samples
h800-l05: > finished creating GPT datasets ...
h800-l05: [after dataloaders are built] datetime: 2024-05-08 16:11:15 
h800-l05: done with setup ...
h800-l05: training ...
h800-l03: (min, max) time across ranks (ms):
h800-l03:     model-and-optimizer-setup ......................: (37482.83, 37511.56)
h800-l03:     train/valid/test-data-iterators-setup ..........: (20937.05, 21603.49)
h800-l05: [before the start of training step] datetime: 2024-05-08 16:11:15 
h800-l05: [2024-05-08 16:11:17,077] [INFO] [checkpointing.py:530:forward] Activation Checkpointing Information
h800-l05: [2024-05-08 16:11:17,078] [INFO] [checkpointing.py:531:forward] ----Partition Activations False, CPU CHECKPOINTING False
h800-l05: [2024-05-08 16:11:17,078] [INFO] [checkpointing.py:532:forward] ----contiguous Memory Checkpointing False with 80 total layers
h800-l05: [2024-05-08 16:11:17,078] [INFO] [checkpointing.py:534:forward] ----Synchronization False
h800-l05: [2024-05-08 16:11:17,078] [INFO] [checkpointing.py:535:forward] ----Profiling time in checkpointing False
h800-l05: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l05: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l05: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l05: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l03: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l03: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l03: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l03: [W ProcessGroupNCCL.cpp:1849] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
h800-l05: [2024-05-08 16:12:52,754] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 148.70 | optimizer_gradients: 1382.88 | optimizer_step: 9373.41
h800-l05: [2024-05-08 16:12:52,754] [INFO] [logging.py:96:log_dist] [Rank 0] step=1, skipped=0, lr=[2.0000000000000002e-07, 2.0000000000000002e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
h800-l05: [2024-05-08 16:12:52,756] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 3789.54 | fwd_microstep: 9564.91 | bwd_microstep: 28535.34 | bwd_inner_microstep: 28534.98 | bwd_allreduce_microstep: 0.01 | step_microstep: 20087.90
h800-l05: [2024-05-08 16:12:52,757] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 9564.60 | bwd: 28535.33 | bwd_inner: 28534.97 | bwd_allreduce: 0.01 | step: 20087.88
h800-l05: steps: 1 loss: 11.1822 iter time (s): 100.526 samples/sec: 1.273
h800-l05: [2024-05-08 16:12:56,296] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | pipe_send_output: 2870.97 | pipe_recv_grad: 12679.44
h800-l03:  iteration        1/  200000 | consumed samples:          128 | consumed tokens:      1048576 | elapsed time per iteration (ms): 100530.4 | learning rate: 2.000E-07 | global batch size:   128 | lm loss: 1.118223E+01 | loss scale: 4096.0 | grad norm: 117.355 | num zeros: 0.0 | actual seqlen:  8192 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.273 | tokens per gpu per second (tgs): 325.951 | TFLOPs: 197.02 |
h800-l03: [Rank 28] (after 1 iterations) memory (MB) | allocated: 22270.90283203125 | max allocated: 45708.46044921875 | reserved: 52218.0 | max reserved: 52218.0
h800-l07: [Rank 8] (after 1 iterations) memory (MB) | allocated: 16258.03466796875 | max allocated: 46534.19189453125 | reserved: 50820.0 | max reserved: 50820.0
h800-l05: [Rank 0] (after 1 iterations) memory (MB) | allocated: 18263.03466796875 | max allocated: 52230.19189453125 | reserved: 57878.0 | max reserved: 57878.0
h800-l05: [Rank 4] (after 1 iterations) memory (MB) | allocated: 16258.03466796875 | max allocated: 48070.19189453125 | reserved: 52356.0 | max reserved: 52356.0
h800-l03: [Rank 24] (after 1 iterations) memory (MB) | allocated: 16258.03466796875 | max allocated: 40390.19189453125 | reserved: 44676.0 | max reserved: 44676.0
h800-l07: [Rank 12] (after 1 iterations) memory (MB) | allocated: 16258.03466796875 | max allocated: 44998.19189453125 | reserved: 49284.0 | max reserved: 49284.0
h800-l09: [Rank 20] (after 1 iterations) memory (MB) | allocated: 16258.03466796875 | max allocated: 41926.19189453125 | reserved: 46212.0 | max reserved: 46212.0
h800-l09: [Rank 16] (after 1 iterations) memory (MB) | allocated: 16258.03466796875 | max allocated: 43462.19189453125 | reserved: 47748.0 | max reserved: 47748.0
h800-l05: [2024-05-08 16:14:04,489] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 945.81 | optimizer_gradients: 421.93 | optimizer_step: 3622.46
h800-l05: [2024-05-08 16:14:04,490] [INFO] [logging.py:96:log_dist] [Rank 0] step=2, skipped=0, lr=[4.0000000000000003e-07, 4.0000000000000003e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
h800-l05: [2024-05-08 16:14:04,491] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 3383.72 | fwd_microstep: 8892.10 | bwd_microstep: 28266.34 | bwd_inner_microstep: 28265.98 | bwd_allreduce_microstep: 0.00 | step_microstep: 11204.19
h800-l05: [2024-05-08 16:14:04,493] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 8891.83 | bwd: 28266.34 | bwd_inner: 28265.98 | bwd_allreduce: 0.00 | step: 11204.19
h800-l05: steps: 2 loss: 11.1879 iter time (s): 68.191 samples/sec: 1.877
h800-l05: [2024-05-08 16:14:04,494] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | pipe_send_output: 27.37 | pipe_recv_grad: 10293.25
h800-l03:  iteration        2/  200000 | consumed samples:          256 | consumed tokens:      2097152 | elapsed time per iteration (ms): 68198.4 | learning rate: 4.000E-07 | global batch size:   128 | lm loss: 1.118786E+01 | loss scale: 4096.0 | grad norm: 116.509 | num zeros: 0.0 | actual seqlen:  8192 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.877 | tokens per gpu per second (tgs): 480.480 | TFLOPs: 290.43 |
h800-l05: [2024-05-08 16:15:08,613] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 343.43 | optimizer_gradients: 245.06 | optimizer_step: 2971.12
h800-l05: [2024-05-08 16:15:08,613] [INFO] [logging.py:96:log_dist] [Rank 0] step=3, skipped=0, lr=[6.000000000000001e-07, 6.000000000000001e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
h800-l05: [2024-05-08 16:15:08,615] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 3053.90 | fwd_microstep: 8886.73 | bwd_microstep: 28330.03 | bwd_inner_microstep: 28329.67 | bwd_allreduce_microstep: 0.00 | step_microstep: 9385.66
h800-l05: [2024-05-08 16:15:08,616] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 8886.46 | bwd: 28330.03 | bwd_inner: 28329.67 | bwd_allreduce: 0.00 | step: 9385.66
h800-l05: steps: 3 loss: 10.1862 iter time (s): 64.544 samples/sec: 1.983
h800-l05: [2024-05-08 16:15:09,045] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | pipe_send_output: 27.42 | pipe_recv_grad: 9157.46
h800-l03:  iteration        3/  200000 | consumed samples:          384 | consumed tokens:      3145728 | elapsed time per iteration (ms): 64550.4 | learning rate: 6.000E-07 | global batch size:   128 | lm loss: 1.018616E+01 | loss scale: 4096.0 | grad norm: 64.881 | num zeros: 0.0 | actual seqlen:  8192 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.983 | tokens per gpu per second (tgs): 507.634 | TFLOPs: 306.84 |
h800-l05: [2024-05-08 16:16:12,856] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 492.36 | optimizer_gradients: 237.46 | optimizer_step: 2647.52
h800-l05: [2024-05-08 16:16:12,857] [INFO] [logging.py:96:log_dist] [Rank 0] step=4, skipped=0, lr=[8.000000000000001e-07, 8.000000000000001e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
h800-l05: [2024-05-08 16:16:12,858] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 3560.25 | fwd_microstep: 8878.62 | bwd_microstep: 28251.89 | bwd_inner_microstep: 28251.53 | bwd_allreduce_microstep: 0.00 | step_microstep: 8666.72
h800-l05: [2024-05-08 16:16:12,859] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 8878.34 | bwd: 28251.89 | bwd_inner: 28251.53 | bwd_allreduce: 0.00 | step: 8666.72
h800-l05: steps: 4 loss: 9.9996 iter time (s): 63.871 samples/sec: 2.004
h800-l05: [2024-05-08 16:16:12,923] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | pipe_send_output: 27.37 | pipe_recv_grad: 10008.62
h800-l03:  iteration        4/  200000 | consumed samples:          512 | consumed tokens:      4194304 | elapsed time per iteration (ms): 63876.3 | learning rate: 8.000E-07 | global batch size:   128 | lm loss: 9.999647E+00 | loss scale: 4096.0 | grad norm: 303.036 | num zeros: 0.0 | actual seqlen:  8192 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.004 | tokens per gpu per second (tgs): 512.992 | TFLOPs: 310.08 |
pdsh@h800-l05: interrupt (one more within 1 sec to abort)
pdsh@h800-l05:  (^Z within 1 sec to cancel pending threads)
pdsh@h800-l05: h800-l05: command in progress
pdsh@h800-l05: h800-l07: command in progress
pdsh@h800-l05: h800-l09: command in progress
pdsh@h800-l05: h800-l03: command in progress
sending SIGTERM to ssh h800-l05
sending signal 15 to h800-l05 [ssh] pid 52543
sending SIGTERM to ssh h800-l07
sending signal 15 to h800-l07 [ssh] pid 52542
sending SIGTERM to ssh h800-l09
sending signal 15 to h800-l09 [ssh] pid 52544
sending SIGTERM to ssh h800-l03
sending signal 15 to h800-l03 [ssh] pid 52545
pdsh@h800-l05: interrupt, aborting.
